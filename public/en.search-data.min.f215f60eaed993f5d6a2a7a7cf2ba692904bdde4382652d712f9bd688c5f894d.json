[{"id":0,"href":"/docs/details/","title":"Details","section":"Docs","content":" ğŸ“š Distributed Systems Lab â€” Detailed Breakdown # This lab is structured as a step-by-step journey through the lifecycle of building a distributed system with microservices.\nğŸ“‚ Reference Project # You will find a reference project for this lab in the following repository: Distributed Systems Project.\nğŸš€ Phase 1: Monolithic Smart Library System # We begin with a monolithic design, where all components of the system reside in a single codebase.\nYouâ€™ll Learn:\nHow to structure a monolithic application Why monoliths become difficult to scale and maintain Building core modules: Users, Catalog, Borrowing Separation of concerns: Controller â†’ Service â†’ Repository layers ğŸ§© Phase 2: Transition to Microservices # Next, we break down the monolith into independently deployable microservices.\nYouâ€™ll Learn:\nHow to decompose services by business capability RESTful API communication between services Managing independent data sources Principles of loose coupling and bounded context ğŸŒ Phase 3: Reverse Proxy with Nginx # We introduce Nginx to centralize and manage access to microservices.\nYouâ€™ll Learn:\nHow reverse proxies work Nginx configuration for routing and load balancing Handling HTTPS and static files with Nginx Benefits of centralized access control and logging ğŸ³ Phase 4: Containerization with Docker # Now we containerize each service to ensure consistency across environments.\nYouâ€™ll Learn:\nWriting Dockerfiles for services Managing containers, images, and networks Creating isolated environments for each service âš™ï¸ Phase 5: Managing with Docker Compose # With more services, orchestration becomes important. Enter Docker Compose.\nYouâ€™ll Learn:\nDefining multi-service environments in docker-compose.yml Networking between services Simplifying development and testing ğŸš¢ Phase 6: Orchestration with Docker Swarm (Optional) # For real-world scalability, we use Docker Swarm to orchestrate services across a cluster.\nYouâ€™ll Learn:\nSetting up a Swarm cluster Deploying services in a distributed fashion Achieving high availability and self-healing systems ğŸ§  Group Presentation on System Design # In this collaborative exercise, teams will tackle real-world system design challenges that scale from hundreds to millions of users. Each team will:\nSelect a Scalable Project: Choose from industry-relevant scenarios like social media timelines, URL shorteners, or distributed search engines Create a Multi-Stage Scaling Plan: Design architecture that evolves through three growth stages (1K â†’ 100K â†’ 1M+ users) Specify Technical Components: Detail infrastructure, databases, caching strategies, API design, and monitoring solutions Present with Diagrams: Deliver a comprehensive presentation with architecture diagrams, justifications for design choices, and analysis of potential failure points Teams will be evaluated on their application of distributed systems principles, scalability considerations, and technical communication skills.\nğŸ“š Group Presentation on Industry Trendy Distributed Systems # Teams will deeply explore modern distributed technologies that power today\u0026rsquo;s scalable applications. For this presentation:\nResearch \u0026amp; Analyze: Thoroughly investigate an assigned distributed system technology (e.g., Cassandra, Kafka, Redis Cluster, Etcd, Consul, CockroachDB, or MinIO) Document Key Aspects: Architecture: Core components and structural design Working Principles: Algorithms and data flow mechanisms High Availability: Failure handling and redundancy approaches Use Cases: Optimal application scenarios and problem domains Integration: How it fits within microservices ecosystems Deliver Engaging Presentation: Share insights with peers and faculty, emphasizing practical applications Presentations should reinforce understanding of consistency models, consensus protocols, data partitioning, and state management in distributed systems.\nğŸ“ Final Outcome # By the end of this lab, you will have:\nBuilt and deployed a functional microservices application Practiced containerization and orchestration Understood the real-world workflow of backend and DevOps engineering Strengthened your system design thinking Ready to build your first distributed system? Letâ€™s dive in! ğŸš€\n"},{"id":1,"href":"/docs/timeline/","title":"Course Timeline","section":"Docs","content":" ğŸ“… Course Timeline \u0026amp; Milestones # This timeline provides a week-by-week breakdown of the course, showing lecture topics and corresponding assignments. The course is designed to progressively build your distributed systems skills from fundamentals to advanced orchestration.\nğŸ—“ï¸ Weekly Schedule # Week Lecture Topics Lab Assignment Deliverables 1 Microservices Fundamentals â€¢ Introduction to distributed systemsâ€¢ Monolithic vs microservices architecture Phase 1: Monolithic Smart Library System Working monolithic application 2-3 Transition to Microservices â€¢ Service decomposition strategiesâ€¢ Inter-service communicationâ€¢ Phase 1 evaluation \u0026amp; feedback Phase 2: Microservices Implementation Multiple independent services with API endpoints 4-5 API Gateway \u0026amp; Reverse Proxy â€¢ Nginx configuration \u0026amp; deploymentâ€¢ Load balancing strategiesâ€¢ Phase 2 evaluation \u0026amp; feedback Phase 3: Nginx Integration Centralized access point with proper routing 6-7 Containerization â€¢ Docker fundamentalsâ€¢ Container networkingâ€¢ Phase 3 evaluation \u0026amp; feedback Phase 4: Docker Implementation Containerized services with proper isolation 8-9 Container Orchestration Basics â€¢ Docker Compose workflowsâ€¢ Multi-container applicationsâ€¢ Phase 4 evaluation \u0026amp; feedback Phase 5: Docker Compose Setup Multi-container application with defined dependencies 10 Advanced Orchestration â€¢ Docker Swarm architectureâ€¢ Service scaling \u0026amp; managementâ€¢ Phase 5 evaluation \u0026amp; feedback Group Presentations: Design \u0026amp; Documentation Project proposal \u0026amp; architecture diagram 11 System Design Principles â€¢ Scalability patternsâ€¢ High availability strategies Group Presentations: Implementation \u0026amp; documentation Progress report \u0026amp; technical documentation 12-13 Group Presentations â€¢ System design presentationsâ€¢ Industry trendy distributed systems showcase Group Presentations: Finalization \u0026amp; presentation prep Final presentation slides \u0026amp; demo 14 Course Review \u0026amp; Final Evaluation â€¢ Best practices reviewâ€¢ Industry application discussion Final Assessment Course completion \u0026amp; project evaluation ğŸ“Š Grading Distribution # Individual Lab Phases (1-5): Each phase will be evaluated based on functionality, code quality, and documentation. Detailed rubric will be published soon. Group Project \u0026amp; Presentation: Assessment will focus on system design principles, technical implementation, and presentation quality. Detailed evaluation criteria will be shared before Week 10. ğŸ” Important Notes # Each phase builds upon the previous one, creating a complete learning journey Evaluations occur regularly to provide feedback and guidance Group work begins in Week 10, allowing sufficient time for comprehensive research and implementation Final presentations should demonstrate both technical mastery and effective communication "},{"id":2,"href":"/docs/prerequisites/","title":"Prerequisites","section":"Docs","content":" ğŸ“‹ Prerequisites # Before diving into the Smart Library System project, itâ€™s important to ensure that students or participants have the foundational knowledge and hands-on exposure to the following areas:\nğŸŒ 1. Basic Understanding of Web Application \u0026amp; Development # Understand the client-server model. Familiarity with how a web application works (browser â†’ server â†’ database). Concept of HTTP verbs: GET, POST, PUT, DELETE, etc. Awareness of RESTful API principles: Resource-oriented endpoints Stateless communication Uniform interface ğŸ”§ 2. API Development in Any Language # Participants should have prior experience with creating APIs in any programming language.\nğŸ’¡ Reference: Check out the ğŸ› ï¸ Tools \u0026amp; Technology Stack section for supported languages and frameworks (Python, Node.js, Java, C#).\nMinimum experience includes:\nRouting \u0026amp; endpoint creation Accepting and parsing JSON input Sending JSON responses Status codes (200 OK, 400 Bad Request, etc.) ğŸ—ƒï¸ 3. Basic Database Understanding # Familiarity with relational databases such as PostgreSQL, MySQL, or MSSQL.\nStudents should know:\nCreating tables and defining schema Performing CRUD operations: SELECT, INSERT, UPDATE, DELETE Concepts like: Primary keys Foreign keys Relationships (one-to-many, many-to-many) ğŸ”Œ 4. Database Integration with Backend # Experience connecting an application to a database, such as:\nUsing ORM (like SQLAlchemy, TypeORM, Hibernate, EF Core) Writing raw SQL queries or using query builders Handling database migrations (bonus) Performing DB transactions inside the application code ğŸ” 5. API Testing Skills # Comfortable with using tools like Postman or Bruno to:\nSend requests to an API Provide JSON body and headers View responses and validate status codes Automate collections for repeated tests ğŸ’¡ For tools, see: API Testing Tools\nğŸ 6. Debugging Experience # The ability to identify and troubleshoot bugs during development.\nThis includes:\nReading and understanding stack traces Using logging tools or breakpoints (e.g., print, console.log, logger.debug()) Fixing common backend issues: DB connection errors Invalid input handling Unhandled exceptions âœ… Note: These skills will ensure a smoother experience throughout the project and allow participants to focus on designing scalable microservices rather than struggling with basic development issues.\n"},{"id":3,"href":"/docs/tools/","title":"Tools","section":"Docs","content":" ğŸ› ï¸ Tools \u0026amp; Technology Stack # This section outlines the recommended tools, frameworks, and platforms used in building and testing the Smart Library System (Microservices Architecture).\nğŸš€ Language \u0026amp; Framework Preferences # Choose any language or framework from below, depending on your expertise or lab instruction:\nğŸ Python # Framework Description FastAPI High-performance, modern API framework using ASGI. Ideal for microservices. Flask Lightweight WSGI framework. Easy to use, good for learning microservice patterns. Django Full-featured web framework. Use with Django REST Framework for API-based microservices. ğŸŒ Node.js # Framework Description Express.js Minimal and flexible framework for building REST APIs. Fast and widely used. â˜• Java # Framework Description Spring Boot Production-grade microservice framework with robust dependency injection and JPA support. ğŸ§± C# # Framework Description .NET Core / ASP.NET Core Cross-platform microservice-ready framework by Microsoft. Supports REST APIs out-of-the-box. ğŸŒ API Testing Tools # Tool Purpose Postman Powerful GUI for testing REST APIs. Bruno Open-source and developer-friendly API testing alternative to Postman. ğŸ§ Preferred Operating System # OS Version Description Ubuntu 22.04 LTS Long-Term Support, stable and widely used in cloud environments. Ubuntu 24.04 LTS (Upcoming/Recent) Updated LTS version with latest packages. ğŸ—ƒï¸ Preferred Databases # Choose your preferred RDBMS per service â€” all options are supported:\nDatabase Notes PostgreSQL Open-source, feature-rich, and widely used in production. MySQL Lightweight, easy to use, and fast. MSSQL Great for enterprise-grade applications using .NET Core. Each microservice will own its own dedicated database instance.\nğŸ” Database Browsing \u0026amp; Inspection Tools # Tool Description DataGrip Universal database IDE by JetBrains (supports all major RDBMS). pgAdmin PostgreSQLâ€™s official web-based admin interface. "},{"id":4,"href":"/docs/phase-1/","title":"Phase 1","section":"Docs","content":" ğŸ“˜ Smart Library System â€“ Monolithic Architecture # Overview # The Smart Library System (Monolithic Version) is a single, unified application that handles all core functionalities: managing users, books, and book loans. This system is ideal for simple deployments where all components are tightly coupled, sharing the same runtime and database.\nğŸ§© Functional Modules # 1. User Management Module # Register a user (students/faculty). Update user profile. Retrieve user info. 2. Book Management Module # Add/update/remove books. View book availability. Search books by title, author, or genre. 3. Loan Management Module # Issue books to users. Return books. View active/past loans. ğŸ›¢ï¸ Unified Database Schema # Table Description users Stores user information. books Stores book catalog details. loans Tracks issued/returned books. All modules interact with this shared relational database, typically PostgreSQL or MySQL.\nğŸ”„ Internal Communication # All module calls happen via function calls or internal classes. Tight coupling between modules. No network-based interaction â€” all components reside in the same codebase and memory space. ğŸ§ª API Documentation (REST Endpoints) # The Smart Library System exposes a comprehensive RESTful API that follows standard HTTP methods and status codes. This API allows external clients (web applications, mobile apps, CLI tools) to interact with the system.\nAPI Design Principles # RESTful Architecture: Resources are accessed via standard HTTP methods (GET, POST, PUT, DELETE) JSON Payloads: All requests and responses use JSON format for data exchange Consistent Naming: Endpoints follow a consistent naming convention (/api/{resource}) Stateless Communication: No client state is stored on the server between requests Proper Status Codes: HTTP status codes indicate success (2xx), client errors (4xx), or server errors (5xx) The following endpoints demonstrate the core functionality of the monolithic library system. You are open to add more APIs to complete this system based on additional requirements or use cases.\nğŸ”¹ User Endpoints # POST /api/users # Create/register a new user.\n{ \u0026#34;name\u0026#34;: \u0026#34;Alice Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;student\u0026#34; } GET /api/users/{id} # Fetch user profile by ID.\nğŸ”¹ Book Endpoints # POST /api/books # Add a new book.\n{ \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 3 } GET /api/books?search=clean # Search for books by title, author, or keyword.\nGET /api/books/{id} # Retrieve detailed information about a specific book.\nResponse:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 3, \u0026#34;available_copies\u0026#34;: 2, \u0026#34;created_at\u0026#34;: \u0026#34;2025-01-15T10:30:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-01-15T10:30:00Z\u0026#34; } PUT /api/books/{id} # Update book information.\nRequest:\n{ \u0026#34;copies\u0026#34;: 5, \u0026#34;available_copies\u0026#34;: 3 } Response:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 5, \u0026#34;available_copies\u0026#34;: 3, \u0026#34;created_at\u0026#34;: \u0026#34;2025-01-15T10:30:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-04-27T01:54:00Z\u0026#34; } DELETE /api/books/{id} # Remove a book from the catalog.\nResponse:\n204 No Content ğŸ”¹ Loan Endpoints # POST /api/loans # Issue a book to a user.\nRequest:\n{ \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 42, \u0026#34;due_date\u0026#34;: \u0026#34;2025-05-15T23:59:59Z\u0026#34; } Response:\n{ \u0026#34;id\u0026#34;: 1001, \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 42, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-04-27T01:54:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-05-15T23:59:59Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34; } POST /api/returns # Return a borrowed book.\nRequest:\n{ \u0026#34;loan_id\u0026#34;: 1001 } Response:\n{ \u0026#34;id\u0026#34;: 1001, \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 42, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-04-27T01:54:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-05-15T23:59:59Z\u0026#34;, \u0026#34;return_date\u0026#34;: \u0026#34;2025-04-30T14:22:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RETURNED\u0026#34; } GET /api/loans/{user_id} # View loan history for a user.\nResponse:\n[ { \u0026#34;id\u0026#34;: 1001, \u0026#34;book\u0026#34;: { \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34; }, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-04-27T01:54:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-05-15T23:59:59Z\u0026#34;, \u0026#34;return_date\u0026#34;: \u0026#34;2025-04-30T14:22:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RETURNED\u0026#34; }, { \u0026#34;id\u0026#34;: 1002, \u0026#34;book\u0026#34;: { \u0026#34;id\u0026#34;: 57, \u0026#34;title\u0026#34;: \u0026#34;Design Patterns\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Erich Gamma et al.\u0026#34; }, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-04-20T09:15:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-05-04T23:59:59Z\u0026#34;, \u0026#34;return_date\u0026#34;: null, \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34; } ] GET /api/loans/overdue # List all overdue loans.\nResponse:\n[ { \u0026#34;id\u0026#34;: 985, \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: 15, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john@example.com\u0026#34; }, \u0026#34;book\u0026#34;: { \u0026#34;id\u0026#34;: 23, \u0026#34;title\u0026#34;: \u0026#34;The Pragmatic Programmer\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Andrew Hunt, David Thomas\u0026#34; }, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-03-15T10:30:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-03-29T23:59:59Z\u0026#34;, \u0026#34;days_overdue\u0026#34;: 29 } ] PUT /api/loans/{id}/extend # Extend the due date for a loan.\nRequest:\n{ \u0026#34;extension_days\u0026#34;: 7 } Response:\n{ \u0026#34;id\u0026#34;: 1002, \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 57, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-04-20T09:15:00Z\u0026#34;, \u0026#34;original_due_date\u0026#34;: \u0026#34;2025-05-04T23:59:59Z\u0026#34;, \u0026#34;extended_due_date\u0026#34;: \u0026#34;2025-05-11T23:59:59Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34;, \u0026#34;extensions_count\u0026#34;: 1 } ğŸ”¹ Statistics Endpoints # GET /api/stats/books/popular # Get the most borrowed books.\nResponse:\n[ { \u0026#34;book_id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;borrow_count\u0026#34;: 28 }, { \u0026#34;book_id\u0026#34;: 57, \u0026#34;title\u0026#34;: \u0026#34;Design Patterns\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Erich Gamma et al.\u0026#34;, \u0026#34;borrow_count\u0026#34;: 23 }, { \u0026#34;book_id\u0026#34;: 23, \u0026#34;title\u0026#34;: \u0026#34;The Pragmatic Programmer\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Andrew Hunt, David Thomas\u0026#34;, \u0026#34;borrow_count\u0026#34;: 19 } ] GET /api/stats/users/active # Get the most active users.\nResponse:\n[ { \u0026#34;user_id\u0026#34;: 15, \u0026#34;name\u0026#34;: \u0026#34;John Smith\u0026#34;, \u0026#34;books_borrowed\u0026#34;: 12, \u0026#34;current_borrows\u0026#34;: 3 }, { \u0026#34;user_id\u0026#34;: 8, \u0026#34;name\u0026#34;: \u0026#34;Emma Johnson\u0026#34;, \u0026#34;books_borrowed\u0026#34;: 10, \u0026#34;current_borrows\u0026#34;: 1 } ] GET /api/stats/overview # Get system overview statistics.\nResponse:\n{ \u0026#34;total_books\u0026#34;: 1250, \u0026#34;total_users\u0026#34;: 430, \u0026#34;books_available\u0026#34;: 1089, \u0026#34;books_borrowed\u0026#34;: 161, \u0026#34;overdue_loans\u0026#34;: 12, \u0026#34;loans_today\u0026#34;: 8, \u0026#34;returns_today\u0026#34;: 5 } âš ï¸ Limitations of Monolithic Design # Hard to scale individual components independently. Tight coupling makes it difficult to change or test modules in isolation. Single point of failure: one bug can crash the entire app. Deployment of small changes requires redeploying the whole system. Technology stack is uniform across all modules. Development becomes slower as the codebase grows. Team coordination becomes challenging with larger teams. "},{"id":5,"href":"/docs/phase-2/","title":"Phase 2","section":"Docs","content":" ğŸ§© Smart Library System â€“ Microservices Architecture # Overview # In the microservices version of the Smart Library System, the application is divided into three independent services â€” each responsible for a specific domain: User, Book, and Loan. Every service has its own database and communicates with others via HTTP APIs (no queues or Kafka involved in this version).\nğŸ§± Services Overview # 1. User Service # Handles registration, profile management, and user-related queries.\nğŸšª REST Base Path: /api/users ğŸ“¦ Owns a user database. 2. Book Service # Manages book inventory, search, and updates to availability.\nğŸšª REST Base Path: /api/books ğŸ“¦ Owns a book database. 3. Loan Service # Issues and returns books by communicating with both User Service and Book Service.\nğŸšª REST Base Path: /api/loans ğŸ“¦ Owns a loan database. ğŸ›¢ï¸ Databases (One per service) # Service Database Tables User Service user_db users Book Service book_db books Loan Service loan_db loans ğŸ”— Inter-Service Communication # Communication Patterns # In this microservices architecture, services communicate with each other through synchronous HTTP/REST APIs. Here\u0026rsquo;s how the communication flows:\nDirect Service-to-Service Communication: The Loan Service directly calls APIs exposed by the User Service and Book Service. For example, when issuing a book, the Loan Service: Calls GET /api/users/{id} to verify the user exists Calls GET /api/books/{id} to check book availability Calls PATCH /api/books/{id}/availability to update book availability Implementation Details # Service URLs: In development, services might use predefined URLs (e.g., http://user-service:8081, http://book-service:8082). HTTP Clients: Services use HTTP clients to make API calls. Circuit Breakers: Implement circuit breakers to handle failures gracefully. Timeout Handling: Set appropriate timeouts for inter-service calls to prevent cascading failures. Example: Loan Creation Flow # When a client sends a request to create a loan:\nClient sends POST /api/loans to the Loan Service Loan Service performs: GET /api/users/{user_id} â†’ User Service â†“ GET /api/books/{book_id} â†’ Book Service â†“ PATCH /api/books/{book_id}/availability â†’ Book Service â†“ INSERT into loan_db.loans â†“ Return response to Client Error Handling # If the User Service is unavailable, the Loan Service returns a 503 Service Unavailable response. If the Book Service is unavailable, the Loan Service returns a 503 Service Unavailable response. If the user doesn\u0026rsquo;t exist, the Loan Service returns a 404 Not Found response. If the book doesn\u0026rsquo;t exist or has no available copies, the Loan Service returns a 400 Bad Request response. No shared database. Each service is data-isolated for decoupling and autonomy.\nğŸ§ª API Documentation (Microservices) # The Smart Library System\u0026rsquo;s microservices architecture exposes RESTful APIs for each service. These APIs follow standard HTTP methods and status codes, allowing clients and other services to interact with the system.\nAPI Design Principles # RESTful Architecture: Resources are accessed via standard HTTP methods (GET, POST, PUT, DELETE) JSON Payloads: All requests and responses use JSON format for data exchange Service Isolation: Each service has its own API namespace and database Stateless Communication: No client state is stored on the server between requests Proper Status Codes: HTTP status codes indicate success (2xx), client errors (4xx), or server errors (5xx) The following endpoints demonstrate the core functionality of each microservice. You are open to add more APIs to complete this system based on additional requirements or use cases.\nğŸ”¹ User Service Endpoints # POST /api/users # Create/register a new user.\nRequest:\n{ \u0026#34;name\u0026#34;: \u0026#34;Alice Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;student\u0026#34; } Response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Alice Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;student\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-05-03T13:30:00Z\u0026#34; } GET /api/users/{id} # Fetch user profile by ID.\nResponse:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Alice Smith\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;student\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-05-03T13:30:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-05-03T13:30:00Z\u0026#34; } PUT /api/users/{id} # Update user information.\nRequest:\n{ \u0026#34;name\u0026#34;: \u0026#34;Alice Johnson\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice.johnson@example.com\u0026#34; } Response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Alice Johnson\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice.johnson@example.com\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;student\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-05-03T13:30:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-05-03T14:15:00Z\u0026#34; } ğŸ”¹ Book Service Endpoints # POST /api/books # Add a new book.\nRequest:\n{ \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 3 } Response:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 3, \u0026#34;available_copies\u0026#34;: 3, \u0026#34;created_at\u0026#34;: \u0026#34;2025-05-03T13:45:00Z\u0026#34; } GET /api/books?search=clean # Search for books by title, author, or keyword.\nResponse:\n{ \u0026#34;books\u0026#34;: [ { \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 3, \u0026#34;available_copies\u0026#34;: 2 }, { \u0026#34;id\u0026#34;: 43, \u0026#34;title\u0026#34;: \u0026#34;Clean Architecture\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780134494166\u0026#34;, \u0026#34;copies\u0026#34;: 2, \u0026#34;available_copies\u0026#34;: 2 } ], \u0026#34;total\u0026#34;: 2, \u0026#34;page\u0026#34;: 1, \u0026#34;per_page\u0026#34;: 10 } GET /api/books/{id} # Retrieve detailed information about a specific book.\nResponse:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 3, \u0026#34;available_copies\u0026#34;: 2, \u0026#34;created_at\u0026#34;: \u0026#34;2025-05-03T13:45:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-05-03T14:30:00Z\u0026#34; } PUT /api/books/{id} # Update book information.\nRequest:\n{ \u0026#34;copies\u0026#34;: 5 } Response:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;9780132350884\u0026#34;, \u0026#34;copies\u0026#34;: 5, \u0026#34;available_copies\u0026#34;: 4, \u0026#34;created_at\u0026#34;: \u0026#34;2025-05-03T13:45:00Z\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-05-03T15:10:00Z\u0026#34; } PATCH /api/books/{id}/availability # Update a book\u0026rsquo;s available copies (used internally by Loan Service during issue/return).\nRequest:\n{ \u0026#34;available_copies\u0026#34;: 4, \u0026#34;operation\u0026#34;: \u0026#34;increment\u0026#34; } Response:\n{ \u0026#34;id\u0026#34;: 42, \u0026#34;available_copies\u0026#34;: 4, \u0026#34;updated_at\u0026#34;: \u0026#34;2025-05-03T15:20:00Z\u0026#34; } DELETE /api/books/{id} # Remove a book from the catalog.\nResponse:\n204 No Content ğŸ”¹ Loan Service Endpoints # POST /api/loans # Issue a book to a user.\nRequest:\n{ \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 42, \u0026#34;due_date\u0026#34;: \u0026#34;2025-06-03T23:59:59Z\u0026#34; } Process:\nValidate user_id via User Service. Validate book_id and availability via Book Service. If all checks pass, reduce the book\u0026rsquo;s available copy count. Response:\n{ \u0026#34;id\u0026#34;: 1001, \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 42, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-05-03T15:30:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-06-03T23:59:59Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34; } POST /api/returns # Return a borrowed book.\nRequest:\n{ \u0026#34;loan_id\u0026#34;: 1001 } Process:\nUpdate loan status. Increment book availability in Book Service. Response:\n{ \u0026#34;id\u0026#34;: 1001, \u0026#34;user_id\u0026#34;: 1, \u0026#34;book_id\u0026#34;: 42, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-05-03T15:30:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-06-03T23:59:59Z\u0026#34;, \u0026#34;return_date\u0026#34;: \u0026#34;2025-05-15T10:45:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RETURNED\u0026#34; } GET /api/loans/user/{user_id} # Get a user\u0026rsquo;s loan history (active and returned books).\nResponse:\n{ \u0026#34;loans\u0026#34;: [ { \u0026#34;id\u0026#34;: 1001, \u0026#34;book\u0026#34;: { \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34; }, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-05-03T15:30:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-06-03T23:59:59Z\u0026#34;, \u0026#34;return_date\u0026#34;: \u0026#34;2025-05-15T10:45:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RETURNED\u0026#34; }, { \u0026#34;id\u0026#34;: 1002, \u0026#34;book\u0026#34;: { \u0026#34;id\u0026#34;: 43, \u0026#34;title\u0026#34;: \u0026#34;Clean Architecture\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34; }, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-05-10T09:15:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-06-10T23:59:59Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34; } ], \u0026#34;total\u0026#34;: 2 } GET /api/loans/{id} # Get details of a specific loan.\nResponse:\n{ \u0026#34;id\u0026#34;: 1001, \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Alice Johnson\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;alice.johnson@example.com\u0026#34; }, \u0026#34;book\u0026#34;: { \u0026#34;id\u0026#34;: 42, \u0026#34;title\u0026#34;: \u0026#34;Clean Code\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Robert C. Martin\u0026#34; }, \u0026#34;issue_date\u0026#34;: \u0026#34;2025-05-03T15:30:00Z\u0026#34;, \u0026#34;due_date\u0026#34;: \u0026#34;2025-06-03T23:59:59Z\u0026#34;, \u0026#34;return_date\u0026#34;: \u0026#34;2025-05-15T10:45:00Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RETURNED\u0026#34; } âœ… Advantages of Microservices # Independent Development: Teams can work on different services simultaneously. Technology Diversity: Each service can use the most appropriate tech stack. Fault Isolation: One service failing doesn\u0026rsquo;t crash the whole application. Scalability: Services can be scaled independently based on demand. Focused Responsibility: Each service has a clear, bounded context. âš ï¸ Trade-offs # Increased Operational Complexity: Managing multiple services requires more sophisticated DevOps. Network Latency: Inter-service communication adds overhead compared to function calls. Data Consistency Challenges: Maintaining consistency across service boundaries is harder. Distributed Debugging: Tracing issues across services requires specialized tooling. Service Discovery: Services need to locate and communicate with each other reliably. "},{"id":6,"href":"/docs/phase-3/","title":"Phase 3","section":"Docs","content":" ğŸŒ Phase 3: Reverse Proxy with Nginx # This phase introduces Nginx as a reverse proxy, acting as the single entry point to route traffic to backend microservices.\nğŸ¯ Youâ€™ll Learn: # What is a reverse proxy and why it\u0026rsquo;s important Routing and load balancing strategies How to serve APIs and static content behind Nginx Configuring HTTPS (SSL termination) ğŸ›  Topics Covered: # Installing and configuring Nginx on Linux nginx.conf structure and virtual hosts Path-based routing: /api/users â†’ User Service /api/books â†’ Book Service /api/loans â†’ Loan Service Static file delivery for frontend (optional) Logging requests centrally Handling 404s and upstream errors "},{"id":7,"href":"/docs/phase-4/","title":"Phase 4","section":"Docs","content":" ğŸ³ Phase 4: Containerization with Docker # We encapsulate each microservice into its own Docker container to achieve environment consistency and portability.\nğŸ¯ Youâ€™ll Learn: # Why containerization is essential in microservice ecosystems How to write Dockerfiles for Python, Node.js, Java, and .NET Core apps How to build and run containers Container networking and volume mounting ğŸ›  Topics Covered: # Writing Dockerfiles for each service Installing dependencies inside containers Exposing ports and configuring environment variables Docker CLI basics: build, run, exec, logs Creating named networks for communication between services "},{"id":8,"href":"/docs/phase-5/","title":"Phase 5","section":"Docs","content":" âš™ï¸ Phase 5: Managing with Docker Compose # With multiple services running, we now use Docker Compose to manage them together through a unified configuration.\nğŸ¯ Youâ€™ll Learn: # Structuring a docker-compose.yml file Defining services, networks, and volumes Scaling services for development/testing Making services talk to each other by name ğŸ›  Topics Covered: # Docker Compose syntax and service definitions Declaring DB containers (e.g., Postgres, MySQL) Health checks and restart policies Local development workflows with Compose Mounting source code with volumes: for live reloading "},{"id":9,"href":"/docs/phase-6/","title":"Phase 6","section":"Docs","content":" ğŸš¢ Phase 6: Orchestration with Docker Swarm (Optional) # Once youâ€™ve mastered local development, we introduce Docker Swarm for distributed service orchestration.\nğŸ¯ Youâ€™ll Learn: # Running your app on a multi-node Docker cluster Service scaling, rolling updates, and high availability Managing load balancing and service discovery ğŸ›  Topics Covered: # Initializing a Docker Swarm (docker swarm init) Creating overlay networks Deploying stacks via docker stack deploy Scaling services with replicas Monitoring service health and logs ğŸ” Optional Lab: Simulate node failure and observe Swarm\u0026rsquo;s self-healing\n"},{"id":10,"href":"/docs/system-design/","title":"Systems Design Problems","section":"Docs","content":" ğŸ—ï¸ System Design at Scale # Overview # In this phase, we\u0026rsquo;ll tackle the challenge of designing systems that can scale from hundreds to millions of users. Students will work in teams to develop comprehensive system architectures that demonstrate an understanding of distributed systems principles in real-world scenarios.\nğŸ“‹ Learning Objectives # Apply distributed systems concepts to practical, industry-relevant problems Design scalable architectures that evolve with increasing user demands Identify and resolve bottlenecks in system performance Make informed trade-offs between consistency, availability, and partition tolerance Document and present technical designs effectively ğŸ”„ Methodology # Each team will:\nSelect a Project: Choose from the example projects listed below Design in Stages: Create a three-stage scaling plan: Stage 1: Initial design for minimal viable product (~1,000 users) Stage 2: Enhanced design for moderate growth (~100,000 users) Stage 3: Full-scale architecture for massive adoption (~1,000,000+ users) Document Components: For each stage, specify: Infrastructure requirements (servers, load balancers, CDNs) Database choices and data modeling approaches Caching strategies and implementation API design and service boundaries Monitoring and observability solutions Present Solutions: Deliver a comprehensive presentation with diagrams, justifications for design choices, and analysis of potential failure points ğŸš€ Example Projects # Each project presents unique scaling challenges that will test your understanding of distributed systems:\nProject Key Challenges 1. Twitter Timeline \u0026amp; Search Real-time updates, high read throughput, efficient search indexing 2. URL Shortener (like Bit.ly) High availability, redirect performance, analytics tracking 3. Personal Finance App (like Mint.com) Data security, third-party integrations, background processing 4. Social Network Data Structure Complex relationships, feed generation, privacy controls 5. Search Engine Key-Value Store Distributed indexing, query optimization, fault tolerance 6. E-commerce Category Rankings Real-time analytics, caching strategies, consistency requirements 7. Web Crawler Distributed work coordination, politeness policies, data processing pipeline ğŸ“š Resources # System Design Primer - Comprehensive resource for system design concepts AWS Scaling Example - Reference implementation for scaling on AWS System Design - Recommended reading for System Design Advanced System Design - Recommended reading for Advance System Design "},{"id":11,"href":"/docs/exploration/","title":"Some Distributed Systems Exploration","section":"Docs","content":" ğŸŒ Distributed Systems Exploration # Distributed systems are the backbone of scalable and fault-tolerant applications. In this phase, we explore 6+ distributed technologies, understanding how they work, what problems they solve, and where they fit in microservices.\nğŸ” Exploration Methodology # For this phase, students will work in collaborative teams to deeply investigate distributed systems technologies. Each team will:\nResearch \u0026amp; Analyze: Thoroughly explore an assigned distributed system technology Document Findings: Create comprehensive technical documentation Present Solutions: Deliver an engaging presentation to peers and faculty Your exploration should focus on these key aspects:\nFocus Area Key Questions to Address ğŸ—ï¸ Architecture How is the system structured? What are its core components? âš™ï¸ Working Principles What algorithms and techniques make it function? How does data flow? ğŸ›¡ï¸ High Availability How does it handle failures? What redundancy mechanisms exist? ğŸ’¼ Use Cases Where is it most effectively applied? What problems does it solve best? ğŸ”„ Integration How does it fit within a microservices ecosystem? Teams will be evaluated on technical depth, presentation quality, and practical insights provided.\nğŸ“¦ 1. Apache Cassandra â€“ Decentralized NoSQL DB # Architecture Highlights:\nPeer-to-peer ring-based system using consistent hashing Eventual consistency with tunable quorum reads/writes Replication across data centers Uses SSTables and Memtables for fast write-heavy workloads Use Case: High-volume write system for analytics or logs.\nâš¡ 2. Apache Kafka â€“ Distributed Event Streaming Platform # Architecture Highlights:\nDistributed commit log with partitioned topics Producer â†’ Kafka Broker â†’ Consumer Message durability via segment files on disk High-throughput stream processing with horizontal scaling Use Case: Decoupled microservice communication, event sourcing, stream analytics.\nğŸ§  3. Redis Cluster â€“ In-Memory Key-Value Store (Distributed Mode) # Architecture Highlights:\nHash-slot-based sharding (16,384 slots) Master-replica architecture with automatic failover (via Sentinel or Cluster) High-speed data access with optional persistence Use Case: Caching for microservices, pub/sub for chat or real-time updates.\nğŸ§® 4. Etcd â€“ Distributed Key-Value Store for Configuration \u0026amp; Coordination # Architecture Highlights:\nStrongly consistent store using Raft consensus Frequently used in Kubernetes for state/config storage Designed for leader election, distributed locking, and configuration management Use Case: Service registry, feature flag storage, cluster coordination.\nğŸ” 5. Consul â€“ Service Discovery and Key/Value Configuration # Architecture Highlights:\nGossip-based peer discovery Offers DNS + HTTP APIs for service registration and health checking KV store with ACLs for configuration sharing Integrates with Envoy for service mesh features Use Case: Discover microservices dynamically, share config globally across services.\nğŸŒ 6. CockroachDB â€“ Distributed SQL Database # Architecture Highlights:\nFollows Google Spanner-inspired architecture Distributed ACID transactions via Raft Multi-region, horizontally scalable SQL Strong consistency with PostgreSQL compatibility Use Case: Global backend for services needing relational queries and strong consistency.\nğŸ“˜ 7. MinIO â€“ Distributed Object Storage (S3-Compatible) # Architecture Highlights:\nDistributed erasure-coded storage Horizontal scaling with node auto-discovery API-compatible with AWS S3 High-performance object store for cloud-native workloads Use Case: Store file uploads, logs, media in a microservices app without relying on AWS.\nğŸ”„ Summary Table # System Type Architecture Keywords Ideal Use Case Cassandra NoSQL DB Peer-to-peer, AP, Sharding High-speed writes, analytics Kafka Event Streaming Broker, Partitioning, Log Messaging, decoupling, events Redis Cluster In-Memory KV Store Sharding, Replication Caching, real-time data Etcd Config Store Raft, Leader Election Cluster state/config (e.g., Kubernetes) Consul Service Discovery/KV Gossip, DNS-based discovery Microservices service registry CockroachDB Distributed SQL DB Raft, ACID, SQL + Scale Transactional workloads, global apps MinIO Object Storage Erasure coding, S3 API Distributed storage for files/media ğŸ§  Concepts Reinforced: # Consistency models (Strong, Eventual, Tunable) Consensus protocols (Raft, Gossip) Data partitioning and replication Service coordination and discovery State management in stateless systems "},{"id":12,"href":"/docs/career-path/","title":"Career Path","section":"Docs","content":" ğŸŒ Career Path # ğŸš€ Why Distributed Systems Matter for Your Career # The skills you\u0026rsquo;re developing in this Distributed Systems course are foundational to some of the most in-demand and well-compensated roles in the technology industry. As systems continue to scale and organizations embrace cloud-native architectures, professionals who understand distributed systems concepts are increasingly valuable.\nğŸ” From Academic Concepts to Industry Roles # This course introduces you to concepts and technologies that directly translate to professional roles such as:\nRole Connection to This Course DevOps Engineer Container orchestration, CI/CD, infrastructure automation Site Reliability Engineer (SRE) System design, fault tolerance, observability Platform Engineer Microservices architecture, API design, service mesh Cloud Engineer/Architect Distributed databases, scalability patterns, multi-region deployments ğŸ’¼ Key Responsibilities in These Roles # If you find yourself particularly interested in the topics covered in this course, you might consider exploring these career paths further:\nDevOps Engineer # Design and implement CI/CD pipelines Manage container orchestration platforms (Kubernetes, Docker Swarm) Automate infrastructure provisioning and configuration Implement monitoring and alerting solutions Optimize deployment workflows and application performance Site Reliability Engineer (SRE) # Design systems for reliability and fault tolerance Implement observability solutions (logging, monitoring, tracing) Create and maintain service level objectives (SLOs) Perform capacity planning and performance optimization Develop automation for incident response and recovery Platform Engineer # Design and build internal developer platforms Create reusable infrastructure components and services Implement service discovery and API gateways Manage service mesh implementations Develop self-service tools for development teams Cloud Engineer/Architect # Design multi-region, highly available architectures Implement cloud-native solutions using managed services Optimize for cost, performance, and security Develop disaster recovery and business continuity plans Create infrastructure as code (IaC) templates ğŸ—ºï¸ Your Learning Roadmap # The topics in this course align closely with the industry-standard DevOps Roadmap, which outlines the skills and technologies professionals need in these roles.\nRemember that the hands-on experience you gain in this lab is invaluableâ€”employers in these fields prioritize practical skills and problem-solving ability over theoretical knowledge alone.\n\u0026ldquo;The most valuable engineers are those who understand not just how to use tools, but why those tools exist and the problems they solve.\u0026rdquo; â€” Google SRE Book\n"},{"id":13,"href":"/docs/about-me/","title":"About Me","section":"Docs","content":" ğŸ‘¨â€ğŸ’» About Me # Hello! Iâ€™m Mohibul Alam, a passionate and hands-on Senior DevOps Engineer with a strong background in cloud infrastructure, automation, and modern backend systems. I bring almost 4 years of experience designing and deploying scalable, secure, and high-performance solutions across AWS, Azure, and on-premises environments.\nMy expertise lies in DevOps automation, container orchestration, observability, and backend development, with a growing interest in building intelligent systems powered by LLMs and AI agents.\nI am deeply involved in teaching and mentoring around cloud-native development and microservice architectures, and I actively contribute to building clean, efficient lab environments for students and professionals to explore real-world DevOps practices.\nğŸ§  What I Do # Architect and automate cloud infrastructure using Terraform, CDK, and CI/CD pipelines Deploy and manage Docker and Kubernetes environments for high availability Monitor systems with tools like Prometheus, Grafana, Tempo, and Loki Develop backend APIs using Python (FastAPI) and serverless platforms like AWS Lambda Build AI-driven applications with LLM frameworks like Promptflow, LlamaIndex, and Semantic Kernel Mentor students and junior engineers in modern software engineering practices ğŸ“ Education # BSc in Software Engineering\nUniversity of Dhaka, Bangladesh\nGraduated: 2022\nğŸ“œ Certifications # ğŸ† AWS Certified Solutions Architect â€“ Associate (SAA-C03) ğŸ›  Certified Kubernetes Application Developer (CKAD) ğŸ”— Find Me Online # LinkedIn GitHub Medium "},{"id":14,"href":"/docs/lecture-resources/lecture-1/cs/","title":"Centralized Systems","section":"Lecture 1: Introduction","content":" Centralized Systems # ğŸ” What is a Centralized System? # A centralized system is an architecture where all processing, data storage, and control functions are concentrated in a single computational entity. This central node handles all requests, manages all resources, and serves as the sole decision-making authority.\nğŸ‘‰ All components, users, and services connect directly to this central entity without intermediate processing or control distribution.\nğŸ« Real-World Analogy # Imagine a traditional classroom setup:\nOne teacher (the central server) stands at the front 30 students (clients) all direct their questions to this one teacher The teacher: ğŸ“š Holds all the knowledge (data storage) ğŸ§  Makes all decisions (processing) ğŸ“ Grades all assignments (computation) ğŸ‘® Maintains classroom discipline (system control) If the teacher is absent, the entire learning process halts - there\u0026rsquo;s no backup system. If too many students ask questions simultaneously, the teacher becomes overwhelmed (system overload).\nï¿½ï¸ Technical Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CENTRAL â”‚ â”‚ SERVER â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â–² â–² â–² â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”¼â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ C1 â”‚ â”‚ C2 â”‚ â”‚ C3 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ Client Client Client ğŸ’» Technical Characteristics # Characteristic Description Technical Implication Single Control Point One entity manages all operations Simplified system management but creates a bottleneck Direct Communication Clients connect directly to the central server Star topology network architecture Resource Concentration All computing resources in one location Requires high-specification hardware at the center Sequential Processing Tasks often processed one after another Limited parallelism capabilities Consistency Single source of truth for all data Strong data consistency without synchronization issues ğŸ¢ Real-World Examples # Traditional Database Systems:\nOracle\u0026rsquo;s single-instance database deployment All queries, transactions, and data modifications go through one database server Uses techniques like connection pooling and query optimization to handle multiple clients Technical limitation: Vertical scaling only (must upgrade the central server for more capacity) Mainframe Computing:\nIBM z/OS systems serving hundreds of terminals Centralized processing unit handles all computation Terminal devices act as input/output only with no local processing Technical components: CICS for transaction processing, VSAM for data storage Single-Server Web Applications:\nLAMP stack (Linux, Apache, MySQL, PHP) on a single server All web requests, database queries, and business logic on one machine Technical challenge: Becomes a performance bottleneck under high traffic ğŸ”„ Evolution to Distributed Systems # Centralized systems evolved toward distributed architectures due to:\nScalability Ceiling - Physical limits to how powerful a single machine can be Reliability Concerns - Unacceptable downtime when the central node fails Geographic Constraints - Latency issues for users far from the central server Resource Utilization - Inefficient use of computing resources (often idle or overloaded) ğŸ†š Comprehensive Comparison # Aspect Centralized System Distributed System Architecture Single processing entity Multiple interconnected nodes Fault Tolerance Low (single point of failure) High (can survive individual failures) Scalability Limited (vertical scaling only) Extensive (horizontal scaling possible) Consistency Strong by default Requires special protocols (CAP theorem) Complexity Lower implementation complexity Higher coordination complexity Latency Higher for distant users Can be optimized with geographic distribution Resource Utilization Often imbalanced Can be optimized across the system Security Centralized control but single target Distributed defense but larger attack surface Example Mainframe computer Cloud computing platform Understanding centralized systems provides an essential foundation for appreciating the innovations and challenges in distributed architectures.\n"},{"id":15,"href":"/docs/lecture-resources/lecture-1/ds/","title":"Distributed Systems","section":"Lecture 1: Introduction","content":" Introduction to Distributed Systems # âœ¨ What is a Distributed System? # A distributed system is a collection of autonomous computing elements that appears to its users as a single coherent system.\nğŸ‘‰ These independent nodes communicate and coordinate their actions by passing messages over a network to achieve a common goal.\nğŸ—ï¸ Technical Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ CLIENT 1 â”‚ â”‚ CLIENT 2 â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ LOAD BALANCER â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ SERVER A â”‚â—„â”€â”€â”€â”€â–ºâ”‚ SERVER B â”‚ â”‚ (Authentication) â”‚ â”‚ (Product Catalog) â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚â—„â”€â”€â”€â”˜ â”‚ DATABASE â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ SERVER C â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ SERVER D â”‚ â”‚ (Order Processing) â”‚ â”‚ (Payment Service) â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ” Real-World Analogy # Imagine a collaborative research project:\nYou and 3 teammates are working together on a complex research paper Each person works from a different location (like separate computers) The work is divided strategically: ğŸ‘¨â€ğŸ’» One person researches and gathers data ğŸ“ Another writes and structures the content ğŸ¨ A third creates diagrams and visualizations âœ… The fourth edits, checks for consistency, and integrates everything You coordinate through messages (Slack, email, calls) - similar to network communication. To your professor, you submit one cohesive final product. ğŸ”µ Despite working independently and asynchronously, the result appears as if created by a unified system. â†’ This illustrates the fundamental concept of a distributed system!\nğŸ—ï¸ Key Characteristics # Characteristic Analogy Technical Description Distribution Team in different locations Nodes are geographically separated and connected via a network Autonomy Independent work Each node has its own processing capabilities and local state Coordination Task planning and updates Nodes synchronize actions through message passing protocols Transparency Seamless final product The complexity of distribution is hidden from end users Fault Tolerance Backup plans if someone gets sick System continues functioning despite individual node failures Scalability Adding more team members Can add more resources to handle increased workload ğŸ’¡ Real-World Implementations # Google Search Engine:\nThousands of servers across global data centers process your query in parallel Different servers handle indexing, ranking, personalization, and serving results Response time: ~200ms despite searching billions of web pages Technical components: MapReduce for processing, GFS for storage, Bigtable for data management Netflix Streaming Platform:\nContent delivery networks (CDNs) distribute video chunks from servers closest to you Adaptive bitrate streaming adjusts quality based on your connection Microservices architecture with 700+ services handling different functions Uses AWS infrastructure across multiple availability zones for redundancy Multiplayer Gaming Ecosystems:\nGame state synchronized across multiple servers and clients Distributed databases track player inventories and progress Load balancers direct players to optimal game instances Consensus algorithms ensure all players see a consistent game world ğŸŒ Core Technical Challenges # Concurrency - Managing simultaneous operations without conflicts Lack of global clock - Coordinating events without perfect time synchronization Independent failures - Handling partial system breakdowns gracefully Network unreliability - Dealing with latency, packet loss, and partitions Understanding these fundamentals will prepare us to explore more advanced distributed systems concepts in upcoming lectures.\n"},{"id":16,"href":"/docs/lecture-resources/lecture-1/types-ds/","title":"Types of Distributed Systems","section":"Lecture 1: Introduction","content":" Types of Distributed Systems # Distributed systems can be categorized based on their architecture, purpose, and how components interact. Each type has distinct characteristics that make it suitable for specific use cases.\n1. Client-Server Systems # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ CLIENT â”‚â”€â”€â”€â”€â–ºâ”‚ SERVER â”‚â—„â”€â”€â”€â”€â”‚ CLIENT â”‚ â”‚ A â”‚ â”‚ â”‚ â”‚ B â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â–² â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CLIENT â”‚ â”‚ C â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Role Separation: Clear division between service providers (servers) and consumers (clients) Centralized Control: Servers manage resources and enforce access policies Request-Response Pattern: Communication follows a request-response cycle Scalability Challenge: Servers can become bottlenecks under high load Technical Implementation # Communication Protocols: HTTP/HTTPS, WebSockets, gRPC Load Distribution: Often uses load balancers to distribute client requests State Management: Servers typically maintain session state Real-World Examples # Web Applications: Gmail, online banking portals API Services: Twitter API, payment gateways Database Clients: SQL clients connecting to database servers Enterprise Applications: SAP, Salesforce 2. Peer-to-Peer (P2P) Systems # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â—„â”€â”€â”€â–ºâ”‚ â”‚â—„â”€â”€â”€â–ºâ”‚ â”‚ â”‚ PEER A â”‚ â”‚ PEER B â”‚ â”‚ PEER C â”‚ â”‚ â”‚â—„â”€â” â”‚ â”‚ â”Œâ”€â–ºâ”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â–ºâ”‚ PEER D â”‚â—„â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Equality: All nodes can function as both clients and servers Decentralization: No central coordination point Resilience: System continues functioning when nodes join or leave Resource Sharing: Computing power, storage, and bandwidth are collectively shared Technical Implementation # Discovery Mechanisms: DHT (Distributed Hash Tables), gossip protocols Routing Algorithms: Chord, Kademlia, Pastry Data Replication: Content is often replicated across multiple peers Security Challenges: Trust establishment without central authority Real-World Examples # File Sharing: BitTorrent, IPFS (InterPlanetary File System) Cryptocurrency Networks: Bitcoin, Ethereum blockchain Communication Tools: Early versions of Skype Distributed Computing: BOINC platform projects 3. Clustered Systems # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CLUSTER MANAGER â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ NODE 1 â”‚ â”‚ NODE 2 â”‚ â”‚ NODE 3 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Physical Proximity: Nodes typically located in the same data center Homogeneous Hardware: Often uses similar or identical machines Shared Resources: Common storage, memory, or processing capabilities High Availability: Designed for fault tolerance and continuous operation Technical Implementation # Resource Management: Uses cluster managers like Kubernetes, Mesos Job Scheduling: Distributes workloads across available nodes Shared Storage: Often uses distributed file systems or SANs Heartbeat Protocols: Monitors node health and manages failover Real-World Examples # High-Performance Computing: Scientific simulation clusters Container Orchestration: Kubernetes clusters running microservices Big Data Processing: Hadoop/Spark clusters Database Clusters: MySQL Cluster, PostgreSQL with replication 4. Grid Computing Systems # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ORGANIZATION â”‚ â”‚ ORGANIZATION â”‚ â”‚ A â”‚ â”‚ B â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚ â”‚NODE â”‚ â”‚NODE â”‚ â”‚ â”‚NODE â”‚ â”‚NODE â”‚ â”‚ â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ GRID MIDDLEWARE â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ RESEARCH PROBLEM â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Geographic Distribution: Resources spread across multiple locations Heterogeneous Resources: Different types of computers and networks Virtual Organizations: Collaboration across institutional boundaries Resource Sharing: Focuses on sharing computing power for large problems Technical Implementation # Grid Middleware: Software like Globus Toolkit that manages resources Job Submission: Uses specialized protocols for submitting computing tasks Security Infrastructure: Certificate-based authentication across domains Data Management: Tools for moving large datasets between grid sites Real-World Examples # Scientific Computing: SETI@home (search for extraterrestrial intelligence) Medical Research: Folding@home (protein folding simulations) Particle Physics: LHC Computing Grid (analyzing CERN data) Climate Modeling: Earth System Grid (climate simulation data) 5. Distributed Databases # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ DATABASE â”‚â—„â”€â”€â”€â”€â–ºâ”‚ DATABASE â”‚ â”‚ NODE 1 â”‚ â”‚ NODE 2 â”‚ â”‚ (Shard A) â”‚ â”‚ (Shard B) â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â””â”€â”€â”€â”€â–ºâ”‚ â”‚â—„â”€â”€â”€â”˜ â”‚ QUERY â”‚ â”‚ ROUTER â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CLIENT â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Data Partitioning: Information divided across multiple nodes (sharding) Replication: Data copied to multiple locations for redundancy Consistency Models: Various approaches to maintaining data consistency Distributed Transactions: Mechanisms for maintaining ACID properties across nodes Technical Implementation # Consensus Algorithms: Paxos, Raft for maintaining consistency Partitioning Strategies: Range-based, hash-based, or directory-based sharding Query Processing: Distributed query execution and optimization CAP Theorem Tradeoffs: Balancing Consistency, Availability, and Partition tolerance Real-World Examples # NoSQL Databases: Cassandra (AP system), MongoDB (CP system) NewSQL Databases: Google Spanner, CockroachDB Time-Series Databases: InfluxDB, TimescaleDB Graph Databases: Neo4j (clustered), Amazon Neptune 6. Distributed File Systems # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ STORAGE â”‚ â”‚ STORAGE â”‚ â”‚ NODE 1 â”‚ â”‚ NODE 2 â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â””â”€â”€â”€â–ºâ”‚ â”‚â—„â”€â”€â”€â”˜ â”‚ METADATA â”‚ â”‚ SERVER â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CLIENT â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Transparent Access: Files appear as if they\u0026rsquo;re on a local system Location Independence: Users don\u0026rsquo;t need to know physical file locations Fault Tolerance: System continues operating despite node failures Scalability: Can add storage nodes to increase capacity Technical Implementation # Chunking: Files split into blocks distributed across nodes Metadata Management: Separate servers track file locations and attributes Caching: Local caching improves performance for frequently accessed files Consistency Protocols: Mechanisms to handle concurrent file access Real-World Examples # Cloud Storage: Amazon S3, Google Cloud Storage Big Data Storage: Hadoop Distributed File System (HDFS) Enterprise Storage: GlusterFS, Ceph Research Systems: Google File System (GFS), Andrew File System (AFS) 7. Cloud-Based Distributed Systems # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CLOUD PROVIDER â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ COMPUTE â”‚ â”‚ STORAGE â”‚ â”‚ DATABASE â”‚ â”‚ â”‚ â”‚ SERVICES â”‚ â”‚ SERVICES â”‚ â”‚ SERVICES â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ MOBILE â”‚ â”‚ WEB â”‚ â”‚ IOT â”‚ â”‚ CLIENTS â”‚ â”‚ CLIENTS â”‚ â”‚ DEVICES â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Elasticity: Resources can scale up or down based on demand Service-Based: Composed of multiple specialized services Multi-Tenancy: Infrastructure shared among multiple customers Pay-Per-Use: Resources billed based on actual consumption Technical Implementation # Virtualization: Uses VMs, containers, or serverless functions Service Orchestration: Automated provisioning and management API Gateways: Manages access to backend services Monitoring \u0026amp; Telemetry: Distributed tracing and logging Real-World Examples # IaaS: Amazon EC2, Microsoft Azure VMs PaaS: Heroku, Google App Engine SaaS: Salesforce, Microsoft 365 Serverless: AWS Lambda, Azure Functions 8. Microservices Architecture # Architecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ USER â”‚ â”‚ PRODUCT â”‚ â”‚ ORDER â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ API GATEWAY â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ CLIENT â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Key Characteristics # Service Independence: Each service can be developed and deployed separately Domain-Focused: Services organized around business capabilities Decentralized Data: Each service typically manages its own data Smart Endpoints, Dumb Pipes: Logic in services, simple communication channels Technical Implementation # Service Discovery: Mechanisms for services to find each other API Gateways: Single entry point for clients Circuit Breakers: Prevent cascading failures Event-Driven Communication: Often uses message queues for asynchronous communication Real-World Examples # E-commerce Platforms: Amazon\u0026rsquo;s service-oriented architecture Streaming Services: Netflix microservices (700+ services) Financial Systems: PayPal\u0026rsquo;s payment processing platform Transportation: Uber\u0026rsquo;s ride-sharing platform "},{"id":17,"href":"/docs/lecture-resources/lecture-1/ms/","title":"Microservices","section":"Lecture 1: Introduction","content":" Microservices Architecture # Definition and Core Concepts # Microservices is an architectural approach where an application is structured as a collection of small, loosely coupled services, each:\nFocused on a specific business capability Independently deployable Communicating through well-defined APIs Owned by small teams This architecture stands in contrast to the traditional monolithic approach where all functionality exists in a single, tightly integrated application.\nArchitecture # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ API GATEWAY â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ USER â”‚ â”‚ PRODUCT â”‚ â”‚ ORDER â”‚ â”‚ PAYMENT â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ USER â”‚ â”‚ PRODUCT â”‚ â”‚ ORDER â”‚ â”‚ PAYMENT â”‚ â”‚ DB â”‚ â”‚ DB â”‚ â”‚ DB â”‚ â”‚ DB â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ï¿½ Real-World Example: Food Delivery Platform # Imagine a food delivery platform like Uber Eats or Foodpanda. In a microservices architecture, this system would be decomposed into specialized services:\nMicroservice Responsibility Key Functions ğŸ‘¤ User Service Identity and profile management Authentication, profiles, preferences ğŸ• Restaurant Service Restaurant data management Menus, hours, locations, ratings ğŸ›’ Order Service Order processing Creation, status tracking, history ğŸ’³ Payment Service Transaction handling Payment processing, refunds, invoicing ğŸšš Delivery Service Logistics management Driver tracking, route optimization, ETA calculation ğŸ“± Notification Service Communication Push notifications, emails, SMS alerts Each service:\nHas its own database optimized for its specific needs Can be developed, deployed, and scaled independently May use different programming languages or frameworks best suited to its purpose Communicates with other services via network protocols (REST, gRPC, message queues) ğŸ—ï¸ Architectural Characteristics # Independence and Autonomy # Technology Heterogeneity: Services can use different tech stacks Resilience Isolation: Failure in one service doesn\u0026rsquo;t crash the entire system Independent Deployment: Services can be updated without system-wide downtime Decentralized Data Management: Each service manages its own data Communication Patterns # Synchronous: Direct service-to-service calls (REST, gRPC) Asynchronous: Event-driven communication via message brokers (Kafka, RabbitMQ) API Gateway: Single entry point that routes requests to appropriate services â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â—„â”€â”€â”€ REST/gRPC (Synchronous) â”€â”€â”€â”€â–ºâ”‚ â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ A â”‚ â”‚ B â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ MESSAGE BROKER â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ (Asynchronous) â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ SERVICE â”‚ â”‚ SERVICE â”‚ â”‚ C â”‚ â”‚ D â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸŒ Microservices as Distributed Systems # Microservices architectures are inherently distributed systems because:\nPhysical Distribution: Services typically run on different machines, containers, or cloud instances Network Communication: Services interact over network boundaries Independent Failure Domains: Each service can fail independently Horizontal Scalability: Services can scale independently based on demand Eventual Consistency: Data consistency challenges across service boundaries This distributed nature introduces challenges that must be addressed:\nNetwork latency and reliability Distributed transactions Service discovery and load balancing Monitoring and observability across services ğŸ’¡ Key Benefits and Challenges # Benefits # Scalability: Independent scaling of components based on demand Agility: Faster development cycles and time-to-market Technological Freedom: Teams can choose optimal technologies Resilience: Isolated failures prevent system-wide outages Organizational Alignment: Services can align with business capabilities and team structures Challenges # Distributed System Complexity: Network failures, latency, consistency issues Operational Overhead: More moving parts to monitor and maintain Service Coordination: Managing dependencies between services Testing Complexity: Integration testing across service boundaries Deployment Complexity: Orchestrating multiple services ğŸ” When to Use Microservices # Microservices are well-suited for:\nLarge, complex applications with clear domain boundaries Systems requiring different scaling needs for different components Organizations with multiple development teams working in parallel Applications needing high availability and resilience They may not be appropriate for:\nSimple applications where the overhead outweighs the benefits Early-stage startups prioritizing speed over scalability Teams without experience in distributed systems ğŸ“š Related Concepts # Domain-Driven Design (DDD): Approach to software development focusing on the core domain Containerization: Technologies like Docker that package services with dependencies Orchestration: Systems like Kubernetes that manage deployment and scaling Service Mesh: Infrastructure layer for service-to-service communication "},{"id":18,"href":"/docs/lecture-resources/lecture-2/","title":"Lecture 2: Proxies in Distributed Systems","section":"Lecture Resources","content":" ğŸŒ Proxies in Distributed Systems # Overview # In this lecture, we explore the critical role of proxies in distributed systems architecture. Proxies serve as intermediaries that facilitate communication between different components, enabling scalability, security, and efficient resource management.\nLearning Objectives # Understand the fundamental concepts of proxies in distributed systems Differentiate between forward and reverse proxies Master the implementation of Nginx as a reverse proxy for microservices Apply proxy patterns to solve real-world distributed systems challenges Topics Covered # Proxy Fundamentals\nDefinition and types of proxies Use cases in modern distributed architectures Reverse Proxies with Nginx\nConfiguration and implementation Load balancing and routing strategies Security enhancements Practical Applications # The concepts covered in this lecture are essential for:\nMicroservice architecture implementation API gateway design Service mesh infrastructure Cloud-native application development "},{"id":19,"href":"/docs/lecture-resources/lecture-2/proxy/","title":"Proxy Fundamentals","section":"Lecture 2: Proxies in Distributed Systems","content":" ğŸ”„ Proxies in Distributed Systems # ğŸ§­ Introduction # Modern distributed systems rely on proxies to manage complexity, enhance security, and improve performance. In microservice architectures, proxies play a crucial role in service discovery, load balancing, and API gateway functionality.\nA proxy serves as an intermediary that sits between clients and servers, intercepting and potentially modifying the communication between them. This pattern enables numerous architectural benefits that we\u0026rsquo;ll explore in this lecture.\nğŸ›¡ï¸ What is a Proxy? # A proxy is an intermediary server that acts on behalf of another entity in a network communication. It intercepts requests, may transform them, and forwards them to the appropriate destination.\nProxies serve several key purposes in distributed systems:\nAbstraction: Hide implementation details of backend services Security: Filter traffic and protect backend services Performance: Cache responses and optimize data transfer Routing: Direct traffic based on content or request patterns Monitoring: Log and analyze traffic patterns ğŸ”„ Forward Proxy # A forward proxy acts on behalf of the client. It sits between clients and the wider internet, forwarding client requests to external servers.\nCharacteristics: # The client is aware of the proxy and must be configured to use it Commonly used for access control, content filtering, and anonymity Hides the client\u0026rsquo;s identity from the server Often deployed in corporate or educational networks Key Use Cases: # Internet access control in organizations Content filtering for compliance or security Anonymizing client traffic for privacy Caching frequently accessed content to improve performance Bypassing geo-restrictions on content ğŸŒ Diagram # INTERNET | | +------------------+ +---------------+ +------------------+ | | | | | | | Client 1 ----+-------\u0026gt;| |-------\u0026gt;| Web Server 1 | | | | | | | +------------------+ | | +------------------+ | | +------------------+ | FORWARD | +------------------+ | | | PROXY | | | | Client 2 ----+-------\u0026gt;| |-------\u0026gt;| Web Server 2 | | | | | | | +------------------+ | | +------------------+ | | +------------------+ | | | | | | | Client 3 ----+-------\u0026gt;| | | | | | +------------------+ +---------------+ ^ | PRIVATE NETWORK ğŸ” How Forward Proxies Work # Client Configuration: The client is explicitly configured to send requests through the proxy Request Interception: The proxy receives the client\u0026rsquo;s request Policy Enforcement: The proxy applies access control and filtering policies Request Forwarding: If allowed, the proxy forwards the request to the destination server Response Processing: The proxy receives the server\u0026rsquo;s response, potentially caches it Client Delivery: The proxy returns the response to the client ğŸ§  Key Concepts to Remember # Forward proxies are client-facing intermediaries They require explicit client configuration They\u0026rsquo;re excellent for access control and content filtering They hide client identity from destination servers Common implementations include Squid, TinyProxy, and NGINX (when configured as a forward proxy) In the next section, we\u0026rsquo;ll explore Reverse Proxies and their critical role in modern microservice architectures.\n"},{"id":20,"href":"/docs/lecture-resources/lecture-2/reverse-proxy/","title":"Reverse Proxy Implementation","section":"Lecture 2: Proxies in Distributed Systems","content":" ğŸ”€ Reverse Proxies in Microservices # ğŸ” Forward Proxy vs Reverse Proxy # Feature Forward Proxy Reverse Proxy Position Between client and internet Between internet and backend servers Client Awareness Configured in client Transparent to client Used For Anonymity, content filtering, caching Load balancing, SSL, routing, caching Hides Client Server Example Tools Squid, TinyProxy Nginx, HAProxy, Apache HTTPD ğŸ”„ What is a Reverse Proxy? # A reverse proxy sits in front of one or more backend servers, intercepting requests from clients before they reach the servers. Unlike a forward proxy that acts on behalf of clients, a reverse proxy acts on behalf of servers.\nKey Characteristics: # Transparent to clients: Clients don\u0026rsquo;t need special configuration Server-side deployment: Managed by server/infrastructure administrators Centralized control: Provides a single entry point to multiple backend services Enhanced security: Shields backend servers from direct exposure Primary Benefits: # Load Balancing: Distribute traffic across multiple servers Security: Hide backend infrastructure and provide WAF capabilities SSL Termination: Handle HTTPS encryption/decryption Caching: Improve performance by storing common responses Compression: Reduce bandwidth usage Authentication: Centralize access control Monitoring: Unified logging and metrics collection âš™ï¸ Why Use Nginx in Microservices? # In a microservice architecture, each service runs on a different port or container. Nginx serves as an ideal reverse proxy because it:\nRoutes requests to appropriate services based on URL paths Centralizes access through a single domain/IP Provides caching and compression for improved performance Enhances security via rate limiting, SSL, and request filtering Offers high performance with low resource consumption Supports WebSockets for real-time communication Enables blue-green deployments and A/B testing ğŸŒ Microservices Architecture with Nginx # INTERNET | | â–¼ +-------------+ | | | NGINX | | REVERSE | | PROXY | | | +------+------+ | +--------------------+--+--+--------------------+ | | | | â–¼ â–¼ â–¼ â–¼ +----------------+ +----------------+ +----------------+ +----------------+ | | | | | | | | | USERS SERVICE | | BOOKS SERVICE | | LOANS SERVICE | | STATIC CONTENT | | :5001 | | :5002 | | :5003 | | | | | | | | | | | +-------+--------+ +-------+--------+ +-------+--------+ +----------------+ | | | â–¼ â–¼ â–¼ +----------------+ +----------------+ +----------------+ | | | | | | | USERS DB | | BOOKS DB | | LOANS DB | | | | | | | +----------------+ +----------------+ +----------------+ ğŸ“¦ Example Nginx Configuration # Let\u0026rsquo;s implement a reverse proxy for a microservice architecture with three services:\nUsers API at http://localhost:5001 Books API at http://localhost:5002 Loans API at http://localhost:5003 Basic Configuration: # # /etc/nginx/nginx.conf or /etc/nginx/conf.d/default.conf server { listen 80; server_name library-app.example.com; # Access logging access_log /var/log/nginx/library-app.access.log; error_log /var/log/nginx/library-app.error.log; # Users Service location /api/users/ { proxy_pass http://localhost:5001/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } # Books Service location /api/books/ { proxy_pass http://localhost:5002/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } # Loans Service location /api/loans/ { proxy_pass http://localhost:5003/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } # Static content location / { root /var/www/html; try_files $uri $uri/ /index.html; } } Understanding the Configuration: # listen 80: Accept HTTP connections on port 80 server_name: Domain name for this virtual host location: Route requests based on URL path proxy_pass: Forward requests to backend services proxy_set_header: Pass important headers to backends ğŸ–¼ï¸ Architecture Diagram # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Client â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ NGINX â”‚ â”‚ ReverseProxyâ”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Users API â”‚ â”‚ Books API â”‚ â”‚ Loans API â”‚ â”‚ :5001 â”‚ â”‚ :5002 â”‚ â”‚ :5003 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ” Advanced Nginx Capabilities # âœ… SSL Termination # Let Nginx handle HTTPS so backend services don\u0026rsquo;t have to:\nserver { listen 443 ssl; server_name library-app.example.com; ssl_certificate /etc/ssl/certs/library-app.crt; ssl_certificate_key /etc/ssl/private/library-app.key; # Modern SSL configuration ssl_protocols TLSv1.2 TLSv1.3; ssl_prefer_server_ciphers on; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384; # HSTS (optional) add_header Strict-Transport-Security \u0026#34;max-age=63072000\u0026#34; always; location /api/ { proxy_pass http://localhost:5000/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } âœ… Load Balancing # Distribute traffic across multiple instances of the same service:\n# Define upstream server groups upstream users_service { server 10.0.0.1:5001 weight=3; server 10.0.0.2:5001 weight=1; server 10.0.0.3:5001 backup; } server { location /api/users/ { proxy_pass http://users_service/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } âœ… Rate Limiting # Control excessive requests to sensitive routes:\n# Define rate limiting zone limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s; server { # Apply rate limiting to specific endpoints location /api/loans/checkout { limit_req zone=api_limit burst=20 nodelay; proxy_pass http://localhost:5003/checkout; } } âœ… Caching # Improve performance for static or repeated content:\n# Define cache location and settings proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=api_cache:10m max_size=1g inactive=60m; server { # Apply caching to book catalog (read-heavy, change-infrequent) location /api/books/catalog { proxy_cache api_cache; proxy_cache_valid 200 10m; proxy_cache_key $scheme$host$request_uri; proxy_pass http://localhost:5002/catalog; # Add cache status header add_header X-Cache-Status $upstream_cache_status; } } ğŸ’» Practical Implementation Steps # Install Nginx:\nsudo apt update sudo apt install nginx Create Configuration:\nsudo nano /etc/nginx/conf.d/library-app.conf # Paste your configuration here Test Configuration:\nsudo nginx -t Apply Configuration:\nsudo systemctl reload nginx Monitor Logs:\nsudo tail -f /var/log/nginx/error.log ğŸ› ï¸ Troubleshooting Common Issues # Symptom Possible Cause Fix 502 Bad Gateway Backend service not running Ensure backend is running and port is correct 504 Gateway Timeout Backend service too slow to respond Increase proxy_read_timeout value Missing Client IP Headers not forwarded properly Add proper proxy_set_header directives Incorrect Routing Path mismatch in proxy_pass Check path handling in location blocks SSL Certificate Issues Certificate misconfiguration Verify certificate paths and permissions Caching Not Working Cache configuration incorrect Check cache keys and paths Debugging Commands: # # Check if Nginx is running systemctl status nginx # Test configuration syntax nginx -t # Check open ports netstat -tulpn | grep nginx # Test backend connectivity curl -v http://localhost:5001/ # Check logs tail -f /var/log/nginx/error.log ğŸ§  Key Concepts to Remember # Reverse proxies are server-side intermediaries that clients connect to directly They hide backend infrastructure from clients They provide centralized control for distributed services Nginx excels as a reverse proxy due to its performance and flexibility Key capabilities include routing, load balancing, SSL termination, and caching Proper header forwarding is essential for microservices to function correctly ğŸ“š Further Learning Resources # Nginx Official Documentation Nginx Cookbook Digital Ocean Nginx Configuration Guide "},{"id":21,"href":"/docs/lecture-resources/lecture-3/","title":"Lecture 3: Containerization with Docker","section":"Lecture Resources","content":" Containerization with Docker # Overview # This lecture introduces Docker, a platform that revolutionizes application development, deployment, and management through containerization technology. Docker enables developers to package applications with all necessary dependencies into standardized units called containers, ensuring consistent behavior across different environments.\nKey Topics # Fundamentals of Docker: Learn what Docker is and how it differs from traditional virtualization technologies Container Architecture: Understand how Docker leverages Linux kernel features (namespaces and cgroups) to create lightweight, isolated environments Docker Ecosystem: Explore Docker images, containers, registries, and the Docker Engine Development Workflow: Learn how to write Dockerfiles, build images, and run containers Practical Applications: Examine real-world use cases for Docker in development, testing, and production environments Learning Objectives # By the end of this lecture, you will be able to:\nUnderstand the technical foundations of containerization Create and manage Docker containers using essential commands Write effective Dockerfiles for application packaging Implement Docker in your development workflow Compare Docker containers with traditional virtual machines Why Docker Matters # Docker addresses the classic \u0026ldquo;it works on my machine\u0026rdquo; problem by providing a consistent, portable runtime environment. This technology has become fundamental in modern development practices, cloud computing, and DevOps, making it an essential skill for today\u0026rsquo;s software engineers.\nFurther Reading # For deeper insights into Docker technologies, check out these articles:\nExploring Lightweight Docker Base Images: Alpine, Slim, and Debian Releases Optimizing Docker Images for Faster Deployments and Efficient Resource Usage Distroless Docker Images Explained: Secure, Lightweight, and Ready for Production "},{"id":22,"href":"/docs/lecture-resources/lecture-3/docker/","title":"Docker","section":"Lecture 3: Containerization with Docker","content":" ğŸ³ Docker Demystified: A Deep Dive into Modern Application Delivery # ğŸ“š Table of Contents # What is Docker? Evolution of Docker: From Linux Kernel to Container Revolution Containers, Images, and Registries Why Docker Matters in the Software Development Life Cycle (SDLC) Docker vs Virtual Machines: A Technical Comparison How Docker Uses the OS Kernel: Namespaces \u0026amp; cgroups User Space vs Kernel Space in Docker Writing a Simple Dockerfile Conclusion ğŸ³ What is Docker? # Docker is a platform for developing, shipping, and running applications inside lightweight containers. It ensures your software runs reliably when moved from one environment to anotherâ€”be it from a developerâ€™s laptop to testing, staging, or production environments.\nIn Simple Terms: # Docker = Standardized Software Environment + Speed + Portability\nğŸ§¬ Evolution of Docker: From Linux Kernel to Container Revolution # Docker wasnâ€™t built from scratch. It evolved by wrapping powerful but complex Linux kernel featuresâ€”namespaces and cgroupsâ€”into an easy-to-use tool.\nğŸ”¹ Linux Namespaces: # Introduced in the Linux kernel to isolate processes, users, network, and filesystems. Each process thinks it\u0026rsquo;s running on a dedicated OS.\nğŸ”¹ Linux Control Groups (cgroups): # These control how much CPU, memory, and I/O resources each group of processes can use.\nğŸ”¹ UnionFS: # A layered filesystem Docker uses to compose images efficiently by stacking file changes.\nASCII Diagram: Traditional vs Dockerized Process # Before Docker: +----------------------+ | Linux Host | |----------------------| | App A, B, C | â† Global processes +----------------------+ With Docker: +----------------------+ | Container A | Isolated PID, FS | | Container B | Own User, Net | | Container C | Limited Resources| +----------------------+ Docker made it all accessible with a simple CLI/API and Docker Engine.\nğŸ“¦ Containers, Images, and Registries # ğŸ”¸ What is a Container? # A container is an isolated execution environment for running applications. It includes the app, libraries, dependencies, and runtimeâ€”but shares the host kernel.\nContainer = App + Dependencies + Libraries + Configs Each container is ephemeral, meaning it can be started, stopped, moved, or deleted quickly.\nğŸ”¸ What is a Docker Image? # A Docker image is a read-only blueprint for a container. It defines:\nWhat the container contains (code, binaries, configs) How the container behaves (start commands) Images are built using a Dockerfile.\nLayers in an Image (UnionFS): # Base Image (e.g., ubuntu:20.04) +----------------------+ | App Dependencies | +----------------------+ | App Source Code | +----------------------+ | Run Instructions | +----------------------+ ğŸ”¸ What is a Docker Registry? # A Docker registry is a storage and distribution system for images.\nDocker Hub: Default public registry Private registries: For enterprise use (e.g., AWS ECR, GitHub Container Registry) You pull images from registries and push them when publishing your own.\n# Pull official nginx image docker pull nginx # Push your image to Docker Hub docker push yourname/myapp:1.0 ğŸ” Why Docker Matters in the Software Development Life Cycle (SDLC) # Docker brings consistency, scalability, and speed to every phase of the SDLC.\nğŸ”¨ 1. Development # Uniform environments across teams Quick setup and teardown of dev environments ğŸ§ª 2. Testing # Test on production-like containers Use parallel, isolated test instances ğŸš€ 3. Deployment # Consistent container runs on any server or cloud Seamless with CI/CD pipelines (GitHub Actions, GitLab CI) ğŸ“ˆ 4. Operations # Scales easily with Kubernetes, Docker Swarm Simplifies monitoring and rolling updates ğŸ†š Docker vs Virtual Machines: A Technical Comparison # ğŸ“Œ Key Differences # Feature Virtual Machine Docker Container Boot Time Minutes Seconds OS Requirements Full Guest OS per VM Shares Host OS Kernel Size GBs MBs Performance Slower (Hypervisor overhead) Near-native Portability Limited High (Run Anywhere) ASCII Diagram: VM vs Docker # Traditional VM: +-------------+ | App | | Guest OS | | Hypervisor | | Host OS | | Hardware | +-------------+ Docker: +-------------+ | App | | Docker Engine | Host OS | | Hardware | +-------------+ ğŸ§  How Docker Uses the OS Kernel: Namespaces \u0026amp; cgroups # Namespaces (Isolation) # Each container gets its own view of the system:\nPID namespace: Unique process tree Net namespace: Own network interfaces Mount namespace: Own filesystem mounts Control Groups (Resource Limits) # Docker sets limits using cgroups:\nCPU shares Memory limits Block I/O constraints âš™ï¸ User Space vs Kernel Space in Docker # ğŸ”¹ Kernel Space: # Manages core OS operations Shared among containers and host ğŸ”¹ User Space: # Where applications run Isolated in each container Diagram: # +----------------------------+ | Kernel Space | â† Shared +----------------------------+ | Container A: User Space | | Container B: User Space | | Container C: User Space | +----------------------------+ Docker containers are isolated in user space, but share the host kernel for efficient resource usage.\nğŸ§¾ Writing a Simple Dockerfile # Letâ€™s package a basic Python app using Docker.\nğŸ“ File Structure # myapp/ â”œâ”€â”€ app.py â””â”€â”€ Dockerfile app.py # print(\u0026#34;Hello from inside Docker!\u0026#34;) Dockerfile # # Start from a Python base image FROM python:3.10-slim # Set working directory WORKDIR /app # Copy source code COPY app.py . # Define container start command CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Build \u0026amp; Run # docker build -t hello-docker . docker run hello-docker ğŸ–¨ï¸ Output:\nHello from inside Docker! âœ… Conclusion # Docker is not just another toolâ€”itâ€™s a paradigm shift in how we build, ship, and run software. By combining decades of operating system research (namespaces, cgroups) with a friendly interface, Docker democratized containerization for developers and enterprises alike.\nWhether you\u0026rsquo;re creating monoliths, microservices, or distributed systems, Docker empowers you with:\nSpeed Consistency Isolation Portability ğŸ“š Further Reading # Docker Docs Namespaces - man7.org Control Groups (cgroups) Dockerfile Reference OCI Image Spec "},{"id":23,"href":"/docs/lecture-resources/lecture-3/docker-lab/","title":"Docker Hands-On Lab","section":"Lecture 3: Containerization with Docker","content":" Docker Hands-On Lab # This lab provides practical exercises to reinforce the Docker concepts covered in the lecture. These hands-on activities will help you build confidence in using Docker for development and deployment scenarios.\nPrerequisites # Docker Engine installed on your system Basic command line familiarity Text editor of your choice Exercise 1: Getting Started with Docker # 1.1 Verify Docker Installation # Ensure Docker is properly installed on your system:\ndocker --version docker info 1.2 Running Your First Container # Let\u0026rsquo;s run an interactive container using the official Ubuntu image:\ndocker run -it --rm ubuntu bash Inside the container, try some commands:\nls cat /etc/os-release apt update Type exit to leave the container.\nExercise 2: Building Custom Images # 2.1 Create a Simple Python Web Application # Create a directory for your project:\nmkdir docker-lab cd docker-lab Create a file named app.py:\nfrom http.server import BaseHTTPRequestHandler, HTTPServer import socket import json class WebServer(BaseHTTPRequestHandler): def _set_response(self, content_type=\u0026#34;text/plain\u0026#34;): self.send_response(200) self.send_header(\u0026#39;Content-type\u0026#39;, content_type) self.end_headers() def do_GET(self): if self.path == \u0026#39;/\u0026#39;: # Serve the main page as plain text self._set_response(\u0026#34;text/plain\u0026#34;) hostname = socket.gethostname() response = f\u0026#34;\u0026#34;\u0026#34; Hello from Docker! This page is being served from a Docker container. Container hostname: {hostname} \u0026#34;\u0026#34;\u0026#34; self.wfile.write(response.encode(\u0026#39;utf-8\u0026#39;)) elif self.path == \u0026#39;/api/info\u0026#39;: # Serve JSON API response self._set_response(\u0026#34;application/json\u0026#34;) data = { \u0026#34;hostname\u0026#34;: socket.gethostname(), \u0026#34;python_version\u0026#34;: socket.python_version(), \u0026#34;container\u0026#34;: True } self.wfile.write(json.dumps(data).encode(\u0026#39;utf-8\u0026#39;)) else: self._set_response() self.wfile.write(b\u0026#34;404 - Not Found\u0026#34;) def run(server_class=HTTPServer, handler_class=WebServer, port=3000): server_address = (\u0026#39;\u0026#39;, port) httpd = server_class(server_address, handler_class) print(f\u0026#34;Starting server on port {port}...\u0026#34;) try: httpd.serve_forever() except KeyboardInterrupt: pass httpd.server_close() print(\u0026#34;Server stopped.\u0026#34;) if __name__ == \u0026#39;__main__\u0026#39;: run() 2.2 Create a Dockerfile # Create a file named Dockerfile:\n# Use Python as the base image FROM python:3.9-slim # Set working directory WORKDIR /app # Copy application file COPY app.py . # Expose the port the app runs on EXPOSE 3000 # Command to run the application CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 2.3 Build and Run Your Image # Build your Docker image:\ndocker build -t my-python-app . Run the container:\ndocker run -d -p 3000:3000 --name web-app my-python-app Test your application:\n# Using curl for plain text response curl http://localhost:3000 # Using curl for JSON API curl http://localhost:3000/api/info 2.4 Explore Your Container # Check the running containers:\ndocker ps View container logs:\ndocker logs web-app Execute commands in the running container:\ndocker exec -it web-app bash Stop and remove your container:\ndocker stop web-app docker rm web-app Exercise 3: Advanced Docker Concepts # 3.1 Docker Networks # Create a custom bridge network:\ndocker network create mynetwork Run containers on the network:\ndocker run -d --name container1 --network mynetwork alpine sleep infinity docker run -d --name container2 --network mynetwork alpine sleep infinity Test container communication:\ndocker exec container1 ping -c 3 container2 List networks and inspect:\ndocker network ls docker network inspect mynetwork Clean up:\ndocker stop container1 container2 docker rm container1 container2 docker network rm mynetwork 3.2 Docker Volumes # Create a named volume:\ndocker volume create mydata Run a container with the volume:\ndocker run -d --name vol-test -v mydata:/data alpine sh -c \u0026#34;while true; do date \u0026gt;\u0026gt; /data/dates.txt; sleep 10; done\u0026#34; Inspect the volume data:\ndocker exec vol-test cat /data/dates.txt Use a different container to access the same volume:\ndocker run --rm -v mydata:/data alpine cat /data/dates.txt Clean up:\ndocker stop vol-test docker rm vol-test docker volume rm mydata Conclusion # This lab has covered:\nRunning containers with Docker Building custom Docker images Using Docker networks and volumes Continue experimenting with Docker and explore other advanced topics like multi-stage builds, container orchestration with Kubernetes, and CI/CD integration.\nChallenge Exercise # Python API with Persistent Data # Create a multi-container application with these components:\nA Python Flask API server that stores data in a persistent volume Connect containers using a custom Docker network Configure the containers to start automatically when Docker starts Here\u0026rsquo;s a starter for your Flask API (app.py):\nfrom flask import Flask, request, jsonify import os import json import socket app = Flask(__name__) DATA_FILE = \u0026#39;/data/items.json\u0026#39; # Create data directory if it doesn\u0026#39;t exist os.makedirs(os.path.dirname(DATA_FILE), exist_ok=True) # Initialize with empty data if file doesn\u0026#39;t exist if not os.path.exists(DATA_FILE): with open(DATA_FILE, \u0026#39;w\u0026#39;) as f: json.dump([], f) @app.route(\u0026#39;/items\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_items(): with open(DATA_FILE, \u0026#39;r\u0026#39;) as f: return jsonify(json.load(f)) @app.route(\u0026#39;/items\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def add_item(): new_item = request.json with open(DATA_FILE, \u0026#39;r\u0026#39;) as f: items = json.load(f) items.append(new_item) with open(DATA_FILE, \u0026#39;w\u0026#39;) as f: json.dump(items, f) return jsonify({\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Item added\u0026#34;}) @app.route(\u0026#39;/info\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_info(): return jsonify({ \u0026#34;hostname\u0026#34;: socket.gethostname(), \u0026#34;items_count\u0026#34;: len(json.load(open(DATA_FILE, \u0026#39;r\u0026#39;))), \u0026#34;container\u0026#34;: True }) if __name__ == \u0026#39;__main__\u0026#39;: app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=5000) "},{"id":24,"href":"/docs/lecture-resources/lecture-3/docker-lab-go/","title":"Docker Lab with Go","section":"Lecture 3: Containerization with Docker","content":" Docker Hands-On Lab with Go # This lab provides practical exercises using Go to reinforce the Docker concepts covered in the lecture.\nPrerequisites # Docker Engine installed on your system Basic command line familiarity Text editor of your choice Exercise 1: Getting Started with Docker # 1.1 Verify Docker Installation # Ensure Docker is properly installed on your system:\ndocker --version docker info 1.2 Running Your First Container # Let\u0026rsquo;s run an interactive container using the official Alpine image:\ndocker run -it --rm alpine sh Inside the container, try some commands:\nls cat /etc/os-release apk update Type exit to leave the container.\nExercise 2: Building a Go Application with Docker # 2.1 Create a Simple Go Web Server # Create a directory for your project:\nmkdir docker-go-lab cd docker-go-lab Create a file named main.go:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) type ServerInfo struct { Hostname string `json:\u0026#34;hostname\u0026#34;` GoVersion string `json:\u0026#34;go_version\u0026#34;` Container bool `json:\u0026#34;container\u0026#34;` } func main() { // Get hostname hostname, err := os.Hostname() if err != nil { hostname = \u0026#34;unknown\u0026#34; } // Define HTTP handlers http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain\u0026#34;) fmt.Fprintf(w, \u0026#34;Hello from Docker!\\n\u0026#34;) fmt.Fprintf(w, \u0026#34;This is a Go application running in a container.\\n\u0026#34;) fmt.Fprintf(w, \u0026#34;Container hostname: %s\\n\u0026#34;, hostname) }) http.HandleFunc(\u0026#34;/api/info\u0026#34;, func(w http.ResponseWriter, r *http.Request) { info := ServerInfo{ Hostname: hostname, GoVersion: \u0026#34;1.20\u0026#34;, // Hardcoded for simplicity, in reality use runtime.Version() Container: true, } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(info) }) // Start server port := 8080 fmt.Printf(\u0026#34;Starting server on port %d...\\n\u0026#34;, port) log.Fatal(http.ListenAndServe(fmt.Sprintf(\u0026#34;:%d\u0026#34;, port), nil)) } 2.2 Create a Go Module # Initialize a Go module:\ngo mod init docker-go-lab 2.3 Create a Dockerfile # Create a file named Dockerfile:\n# Build stage FROM golang:1.20-alpine AS build WORKDIR /app # Copy go module files first for better caching COPY go.mod ./ # Copy source code COPY main.go ./ # Build the application RUN go build -o server . # Runtime stage FROM alpine:latest WORKDIR /app # Copy the binary from the build stage COPY --from=build /app/server /app/server # Expose the port EXPOSE 8080 # Run the application CMD [\u0026#34;/app/server\u0026#34;] 2.4 Build and Run Your Image # Build your Docker image:\ndocker build -t go-web-app . Run the container:\ndocker run -d -p 8080:8080 --name go-app go-web-app Access your application:\n# Using curl curl http://localhost:8080 # Using curl for JSON API curl http://localhost:8080/api/info You can also visit http://localhost:8080 in your browser.\n2.5 Explore Your Container # Check the running containers:\ndocker ps View container logs:\ndocker logs go-app Execute commands in the running container:\ndocker exec -it go-app sh Stop and remove your container:\ndocker stop go-app docker rm go-app Exercise 3: Multi-Stage Builds and Optimization # 3.1 Create an Optimized Dockerfile # Let\u0026rsquo;s create an even more optimized Dockerfile that produces a smaller image:\n# Build stage FROM golang:1.20-alpine AS build WORKDIR /app # Copy go module files COPY go.mod ./ COPY main.go ./ # Build with optimizations RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags=\u0026#34;-w -s\u0026#34; -o server . # Runtime stage - using scratch (empty) image FROM scratch # Copy SSL certificates for HTTPS requests COPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ WORKDIR /app # Copy the binary from the build stage COPY --from=build /app/server /app/server # Expose the port EXPOSE 8080 # Run the application CMD [\u0026#34;/app/server\u0026#34;] Build and tag this optimized image:\ndocker build -t go-web-app:optimized -f Dockerfile.optimized . Compare the image sizes:\ndocker images | grep go-web-app Exercise 4: Docker Networks and Volumes with Go # 4.1 Create a Stateful Go Application # Create a new file named stateful.go:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;sync\u0026#34; ) // Counter represents a simple counter with mutex for concurrent access type Counter struct { Value int `json:\u0026#34;value\u0026#34;` Path string `json:\u0026#34;-\u0026#34;` mu sync.Mutex } // NewCounter creates a new counter and loads its value from file if available func NewCounter(dataPath string) *Counter { c := \u0026amp;Counter{ Value: 0, Path: dataPath, } // Create directory if it doesn\u0026#39;t exist dir := filepath.Dir(dataPath) if err := os.MkdirAll(dir, 0755); err != nil { log.Printf(\u0026#34;Error creating directory: %v\u0026#34;, err) } // Try to load existing counter value if data, err := ioutil.ReadFile(dataPath); err == nil { var storedCounter Counter if err := json.Unmarshal(data, \u0026amp;storedCounter); err == nil { c.Value = storedCounter.Value } } return c } // Increment increases the counter and saves it func (c *Counter) Increment() int { c.mu.Lock() defer c.mu.Unlock() c.Value++ // Save to file data, err := json.Marshal(c) if err != nil { log.Printf(\u0026#34;Error marshaling counter: %v\u0026#34;, err) return c.Value } if err := ioutil.WriteFile(c.Path, data, 0644); err != nil { log.Printf(\u0026#34;Error writing counter: %v\u0026#34;, err) } return c.Value } // GetValue returns the current value func (c *Counter) GetValue() int { c.mu.Lock() defer c.mu.Unlock() return c.Value } func main() { // Create a counter that persists to a file counter := NewCounter(\u0026#34;/data/counter.json\u0026#34;) // Get hostname for display hostname, _ := os.Hostname() // Define HTTP handlers http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain\u0026#34;) fmt.Fprintf(w, \u0026#34;Hello from Docker!\\n\u0026#34;) fmt.Fprintf(w, \u0026#34;This is a stateful Go application with persistence.\\n\u0026#34;) fmt.Fprintf(w, \u0026#34;Container hostname: %s\\n\u0026#34;, hostname) fmt.Fprintf(w, \u0026#34;Current count: %d\\n\u0026#34;, counter.GetValue()) fmt.Fprintf(w, \u0026#34;Visit /increment to increase the counter.\\n\u0026#34;) }) http.HandleFunc(\u0026#34;/increment\u0026#34;, func(w http.ResponseWriter, r *http.Request) { newValue := counter.Increment() w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(map[string]interface{}{ \u0026#34;value\u0026#34;: newValue, \u0026#34;hostname\u0026#34;: hostname, }) }) // Start server port := 8080 fmt.Printf(\u0026#34;Starting server on port %d with data persistence...\\n\u0026#34;, port) log.Fatal(http.ListenAndServe(fmt.Sprintf(\u0026#34;:%d\u0026#34;, port), nil)) } Create a Dockerfile for the stateful application:\nFROM golang:1.20-alpine AS build WORKDIR /app COPY stateful.go . RUN go build -o stateful-app stateful.go FROM alpine:latest WORKDIR /app COPY --from=build /app/stateful-app . # Create a volume mount point VOLUME /data EXPOSE 8080 CMD [\u0026#34;/app/stateful-app\u0026#34;] 4.2 Run with Persistent Volume # Build the stateful application:\ndocker build -t go-stateful-app -f Dockerfile.stateful . Create a volume:\ndocker volume create go-app-data Run the container with the volume:\ndocker run -d -p 8080:8080 -v go-app-data:/data --name stateful-app go-stateful-app Test the persistence:\nVisit http://localhost:8080/increment several times Stop and remove the container Start a new container with the same volume Verify the counter value persisted # Stop and remove the container docker stop stateful-app docker rm stateful-app # Start a new container with the same volume docker run -d -p 8080:8080 -v go-app-data:/data --name stateful-app-2 go-stateful-app # Check if the counter value persisted curl http://localhost:8080 Conclusion # This lab has covered:\nRunning containers with Docker Building Go applications for containerized environments Using multi-stage builds for optimization Working with Docker volumes for persistence These practices are essential for developing modern, containerized Go applications.\nChallenge Exercise # Build a multi-container application with these components:\nA Go API server that handles requests A separate data storage container Connect them using Docker networks for communication Implement proper volume mounts for data persistence Both containers should be configured to restart automatically if they crash or if Docker restarts.\n"},{"id":25,"href":"/docs/lecture-resources/lecture-3/docker-cheatsheet/","title":"Docker Command Cheat Sheet","section":"Lecture 3: Containerization with Docker","content":" Docker Command Cheat Sheet # This cheat sheet provides a quick reference for commonly used Docker commands.\nDocker System Commands # Command Description docker version Display Docker version information docker info Display system-wide information docker system prune Remove all unused containers, networks, images, and build cache docker system df Show Docker disk usage Docker Image Commands # Command Description docker images List all images docker pull \u0026lt;image-name\u0026gt; Pull an image from registry docker build -t \u0026lt;name:tag\u0026gt; \u0026lt;path\u0026gt; Build an image from Dockerfile docker rmi \u0026lt;image-id\u0026gt; Remove an image docker image prune Remove unused images docker tag \u0026lt;source-image\u0026gt; \u0026lt;target-image\u0026gt; Create a tag for image docker history \u0026lt;image-name\u0026gt; Show history of an image docker save -o \u0026lt;file.tar\u0026gt; \u0026lt;image-name\u0026gt; Save image to a tar archive docker load -i \u0026lt;file.tar\u0026gt; Load image from a tar archive docker inspect \u0026lt;image-id\u0026gt; Display detailed information about an image Docker Container Commands # Command Description docker ps List running containers docker ps -a List all containers (including stopped) docker run \u0026lt;image-name\u0026gt; Run a container docker run -it \u0026lt;image-name\u0026gt; \u0026lt;command\u0026gt; Run container interactively docker run -d \u0026lt;image-name\u0026gt; Run container in detached mode docker run -p \u0026lt;host-port\u0026gt;:\u0026lt;container-port\u0026gt; \u0026lt;image-name\u0026gt; Run container with port mapping docker run -v \u0026lt;host-path\u0026gt;:\u0026lt;container-path\u0026gt; \u0026lt;image-name\u0026gt; Run container with volume mount docker run --name \u0026lt;container-name\u0026gt; \u0026lt;image-name\u0026gt; Run container with a specific name docker run --rm \u0026lt;image-name\u0026gt; Run container and remove it when it exits docker stop \u0026lt;container-id\u0026gt; Stop a running container docker start \u0026lt;container-id\u0026gt; Start a stopped container docker restart \u0026lt;container-id\u0026gt; Restart a container docker rm \u0026lt;container-id\u0026gt; Remove a container docker rm -f \u0026lt;container-id\u0026gt; Force remove a running container docker exec -it \u0026lt;container-id\u0026gt; \u0026lt;command\u0026gt; Execute a command in a running container docker logs \u0026lt;container-id\u0026gt; Fetch the logs of a container docker logs -f \u0026lt;container-id\u0026gt; Follow log output of a container docker cp \u0026lt;container-id\u0026gt;:\u0026lt;container-path\u0026gt; \u0026lt;host-path\u0026gt; Copy files from container to host docker cp \u0026lt;host-path\u0026gt; \u0026lt;container-id\u0026gt;:\u0026lt;container-path\u0026gt; Copy files from host to container docker stats Display live container resource usage docker inspect \u0026lt;container-id\u0026gt; Display detailed information about a container docker top \u0026lt;container-id\u0026gt; Display the running processes of a container docker update --memory \u0026lt;limit\u0026gt; \u0026lt;container-id\u0026gt; Update container resources Docker Network Commands # Command Description docker network ls List all networks docker network create \u0026lt;network-name\u0026gt; Create a network docker network rm \u0026lt;network-name\u0026gt; Remove a network docker network inspect \u0026lt;network-name\u0026gt; Display detailed information about a network docker network connect \u0026lt;network-name\u0026gt; \u0026lt;container-id\u0026gt; Connect a container to a network docker network disconnect \u0026lt;network-name\u0026gt; \u0026lt;container-id\u0026gt; Disconnect a container from a network docker network prune Remove all unused networks Docker Volume Commands # Command Description docker volume ls List all volumes docker volume create \u0026lt;volume-name\u0026gt; Create a volume docker volume rm \u0026lt;volume-name\u0026gt; Remove a volume docker volume inspect \u0026lt;volume-name\u0026gt; Display detailed information about a volume docker volume prune Remove all unused volumes Dockerfile Instructions # Instruction Description FROM Set base image WORKDIR Set working directory COPY Copy files from host to container ADD Copy files from host or URL to container RUN Execute command during build ENV Set environment variable ARG Define build-time variable EXPOSE Expose port VOLUME Create mount point CMD Default command for container ENTRYPOINT Configure container executable LABEL Add metadata to image USER Set user for container HEALTHCHECK Check container health Common Docker Run Options # Option Description -d, --detach Run container in background -e, --env Set environment variables -p, --publish Publish container\u0026rsquo;s port to the host -v, --volume Bind mount a volume --name Assign a name to the container --network Connect to a network --restart Restart policy (no, on-failure, always, unless-stopped) --rm Remove container when it exits -it Interactive mode with TTY --memory Memory limit --cpus Number of CPUs Docker Command Examples # Running a Nginx Web Server # docker run -d -p 8080:80 --name my-nginx nginx Building a Custom Image # docker build -t my-app:1.0 . Running a Container with Environment Variables # docker run -d -p 3000:3000 -e NODE_ENV=production --name my-node-app my-node-app Creating a Docker Network and Connecting Containers # docker network create my-network docker run -d --name db --network my-network mongo docker run -d --name app --network my-network -p 8000:8000 my-app Volume Mounting for Persistence # docker run -d -p 27017:27017 -v mongodb-data:/data/db --name mongodb mongo Running a MySQL Container with Environment Variables # docker run -d -p 3306:3306 \\ -e MYSQL_ROOT_PASSWORD=secretpassword \\ -e MYSQL_DATABASE=mydb \\ -e MYSQL_USER=user \\ -e MYSQL_PASSWORD=password \\ -v mysql-data:/var/lib/mysql \\ --name mysql \\ mysql:8.0 "},{"id":26,"href":"/docs/lecture-resources/lecture-3/docker-best-practices/","title":"Docker Best Practices","section":"Lecture 3: Containerization with Docker","content":" Docker Best Practices # This document outlines recommended practices for effectively developing, deploying, and managing Docker containers in production environments.\nImage Building Best Practices # Use Specific Base Image Tags # Always use specific version tags for base images rather than latest to ensure consistency and reproducibility.\n# Bad practice FROM node:latest # Good practice FROM node:18.16.0-slim Keep Images Small # Use lightweight base images (Alpine, slim variants) Minimize layers by combining related commands Remove unnecessary files in the same layer they were created # Bad practice FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y python3 RUN apt-get install -y python3-pip RUN pip install requests # Good practice FROM python:3.11-alpine RUN pip install --no-cache-dir requests Use Multi-Stage Builds # Use multi-stage builds to separate build-time dependencies from runtime dependencies, resulting in smaller final images.\n# Build stage FROM node:18 AS build WORKDIR /app COPY package*.json ./ RUN npm ci COPY . . RUN npm run build # Production stage FROM node:18-alpine WORKDIR /app COPY --from=build /app/dist ./dist COPY --from=build /app/package*.json ./ RUN npm ci --only=production EXPOSE 3000 CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] Leverage Build Cache Effectively # Order Dockerfile instructions from least to most frequently changing to optimize build cache usage.\n# Optimal order for caching FROM node:18-alpine WORKDIR /app # Rarely changes COPY package*.json ./ RUN npm ci # More frequently changes COPY . . RUN npm run build CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] Avoid Running as Root # Run containers with a non-root user for security.\nFROM node:18-alpine WORKDIR /app COPY --chown=node:node . . USER node CMD [\u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34;] Container Runtime Best Practices # Use Environment Variables for Configuration # Externalize configuration using environment variables.\nFROM node:18-alpine WORKDIR /app COPY . . CMD [\u0026#34;node\u0026#34;, \u0026#34;server.js\u0026#34;] # Run with: docker run -e DB_HOST=mongodb -e API_KEY=12345 my-app Set Health Checks # Define health checks to enable automated monitoring and container replacement.\nFROM nginx:alpine COPY ./config/nginx.conf /etc/nginx/conf.d/default.conf HEALTHCHECK --interval=30s --timeout=3s \\ CMD curl -f http://localhost/ || exit 1 Implement Proper Logging # Configure applications to log to stdout/stderr for integration with Docker\u0026rsquo;s logging system.\n# Application should log to stdout/stderr # Then view logs with: docker logs container_name Define Resource Limits # Always set resource constraints for containers in production.\n# Set memory and CPU limits docker run --memory=512m --cpus=0.5 my-application Use Volumes for Persistent Data # Use named volumes for data that needs to persist beyond container lifecycle.\n# Create a named volume docker volume create my-data # Use the volume docker run -v my-data:/app/data my-application Security Best Practices # Scan Images for Vulnerabilities # Regularly scan your Docker images for known vulnerabilities.\n# Example using Trivy trivy image myapp:1.0 Sign and Verify Images # Sign your images and verify signatures before deployment for supply chain security.\n# Sign image docker trust sign myapp:1.0 # Only run verified images docker trust inspect --pretty myapp:1.0 Keep Base Images Updated # Regularly update base images to include security patches.\n# Pull latest version of the specific tag you use docker pull node:18-alpine Limit Capabilities # Run containers with minimal required Linux capabilities.\ndocker run --cap-drop=ALL --cap-add=NET_BIND_SERVICE my-app Use Read-Only Filesystem # Mount the container filesystem as read-only when possible.\ndocker run --read-only my-application Development Workflow Best Practices # Use Docker for Local Development # Create development-specific Dockerfiles for your application.\n# Dockerfile.dev FROM node:18-alpine WORKDIR /app COPY package*.json ./ RUN npm install COPY . . CMD [\u0026#34;npm\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;dev\u0026#34;] Run with:\ndocker build -f Dockerfile.dev -t myapp-dev . docker run -v $(pwd)/src:/app/src -p 3000:3000 myapp-dev Implement CI/CD for Docker Images # Automate building, testing, and deploying Docker images.\n# Example GitHub Actions workflow name: Docker Build and Push on: push: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Build and test run: | docker build -t myapp:test . docker run myapp:test npm test - name: Push to registry if: success() run: | echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin docker tag myapp:test myorg/myapp:${{ github.sha }} docker push myorg/myapp:${{ github.sha }} Use Docker BuildKit # Enable BuildKit for faster, more efficient builds.\n# Enable BuildKit export DOCKER_BUILDKIT=1 docker build . Monitoring and Maintenance # Set Up Container Monitoring # Implement monitoring for all Docker containers in production.\n# Example using Prometheus and cAdvisor docker run -d \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --publish=8080:8080 \\ --name=cadvisor \\ gcr.io/cadvisor/cadvisor:latest Log Aggregation # Implement centralized logging for all containers.\n# Run ELK stack for log aggregation docker run -d \\ --name elasticsearch \\ -p 9200:9200 \\ -e \u0026#34;discovery.type=single-node\u0026#34; \\ elasticsearch:7.14.0 docker run -d \\ --name logstash \\ --link elasticsearch \\ -v \u0026#34;$PWD/logstash.conf:/usr/share/logstash/pipeline/logstash.conf\u0026#34; \\ logstash:7.14.0 docker run -d \\ --name kibana \\ --link elasticsearch \\ -p 5601:5601 \\ kibana:7.14.0 Regular Cleanup # Implement regular cleanup of unused Docker objects.\n# Create a cleanup cron job 0 0 * * * docker system prune -af --volumes \u0026gt; /var/log/docker-cleanup.log 2\u0026gt;\u0026amp;1 Production Deployment Considerations # Use Orchestration Tools # For production environments with multiple containers, use orchestration tools like Kubernetes or Docker Swarm.\nImplement Rolling Updates # Configure services for zero-downtime updates.\n# Docker Swarm example docker service create --name myapp --replicas 3 myapp:1.0 docker service update --image myapp:1.1 myapp Backup Strategies # Implement regular backups for all persistent data.\n# Example backup script for a volume docker run --rm -v my_volume:/data -v $(pwd)/backup:/backup alpine \\ tar -czf /backup/my_volume_$(date +%Y%m%d_%H%M%S).tar.gz -C /data . Conclusion # Following these best practices will help you build more secure, efficient, and maintainable Docker-based applications. Remember that Docker is constantly evolving, so it\u0026rsquo;s important to stay updated with the latest recommendations and security advisories.\n"},{"id":27,"href":"/docs/lecture-resources/lecture-4/__index/","title":"Lecture 4: Docker Swarm","section":"Lecture Resources","content":" How Docker works # !Docker\nWhat problems might we face if we deploy our production application in standalone Docker container mode? Running a production application using just standalone Docker containersâ€”without orchestrationâ€”can lead to several challenges and operational risks: Single Point of Failure If the container crashes due to an application bug, out-of-memory error, or underlying issue, thereâ€™s no automatic recovery. The application will remain down until someone manually restarts the container, causing unwanted downtime.\nLack of Auto-Scaling In standalone mode, Docker does not provide any built-in mechanism to scale horizontally based on CPU, memory, or user demand. If traffic increases suddenlyâ€”such as during a flash sale or peak usage hourâ€”your single container may become overwhelmed and unresponsive, leading to degraded performance or complete service unavailability.\nNo Load Balancing Without orchestration, you typically run a single container instance. This limits the applicationâ€™s ability to serve multiple users effectively. If you try to run multiple containers manually, thereâ€™s no built-in load balancing across them.\nHost Dependency If the physical or virtual host machine goes downâ€”whether due to hardware failure, a crash, or maintenanceâ€”all containers on that host go down with it. There\u0026rsquo;s no failover or container migration, which makes high availability difficult.\nNo Health Check \u0026amp; Auto-Restart Logic While Docker does support basic restart policies, it lacks intelligent health monitoring. For example, if your container is running but the app inside is stuck or returning errors, Docker wonâ€™t automatically restart it unless it crashes completely.\nNo Centralized Management or Logging Managing multiple containers across environments becomes tedious. You donâ€™t get a central dashboard, logging, or monitoring unless you integrate separate tools, which increases operational complexity.\nManual Networking and Configuration Networking between containers must be configured manually, which can become complex in multi-container setups. Also, secrets management, config updates, and port management lack the dynamic capabilities provided by orchestrators.\n"}]